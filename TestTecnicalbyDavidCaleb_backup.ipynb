{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccb01403",
   "metadata": {},
   "source": [
    "# 🛒 Sistema de Recomendación para E-Commerce\n",
    "**Prueba Técnica - Lead Data Scientist**  \n",
    "**Autor:** David Caleb  \n",
    "**Fecha:** Octubre 2025\n",
    "\n",
    "---\n",
    "\n",
    "## 📋 Tabla de Contenidos\n",
    "\n",
    "1. [Introducción y Objetivos](#seccion-1-introduccion)\n",
    "2. [Metodología CRISP-DM](#seccion-2-metodologia)\n",
    "3. [Análisis Exploratorio de Datos (EDA)](#seccion-3-eda)\n",
    "4. [Preprocesamiento y Preparación](#seccion-4-preprocesamiento)\n",
    "5. [Sistema de Recomendación](#seccion-5-sistema-recomendacion)\n",
    "6. [Evaluación de Modelos](#seccion-6-evaluacion)\n",
    "7. [Sistema de Actualización](#seccion-7-actualizacion)\n",
    "8. [Monitorización Continua](#seccion-8-monitorizacion)\n",
    "9. [Framework de A/B Testing](#seccion-9-ab-testing)\n",
    "10. [Conclusiones y Recomendaciones](#seccion-10-conclusiones)\n",
    "\n",
    "---\n",
    "\n",
    "<a id=\"seccion-1-introduccion\"></a>\n",
    "## 🎯 1. Introducción y Objetivos\n",
    "\n",
    "### Contexto de Negocio\n",
    "La empresa de e-commerce busca **optimizar la experiencia del cliente** mediante un sistema de recomendación personalizado que:\n",
    "- Aumente las conversiones\n",
    "- Mejore el engagement del usuario\n",
    "- Incremente el valor promedio del pedido\n",
    "\n",
    "### Objetivos del Proyecto\n",
    "\n",
    "**Técnicos:**\n",
    "- Implementar múltiples algoritmos de recomendación\n",
    "- Alcanzar Precision@10 mayor a 0.003\n",
    "- Manejar el problema de cold start\n",
    "- Gestionar datos dispersos\n",
    "\n",
    "**De Negocio:**\n",
    "- Aumentar CTR en recomendaciones en 15%\n",
    "- Incrementar tasa de conversión en 10%\n",
    "- Mejorar retención de clientes\n",
    "\n",
    "### Alcance\n",
    "\n",
    "**En Scope:** Análisis exploratorio, 5 algoritmos, evaluación offline, plan de deployment\n",
    "\n",
    "**Out of Scope:** Deep Learning, procesamiento en tiempo real, NLP\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e498c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cab2d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning - scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# Sparse matrices - scipy\n",
    "from scipy.sparse import csr_matrix, coo_matrix\n",
    "from scipy import stats\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure visualization styles\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0640221c",
   "metadata": {},
   "source": [
    "<a id=\"seccion-2-metodologia\"></a>\n",
    "# 📊 2. Metodología: CRISP-DM\n",
    "\n",
    "---\n",
    "\n",
    "## Framework de Trabajo\n",
    "\n",
    "Este proyecto sigue la metodología **CRISP-DM** (Cross-Industry Standard Process for Data Mining):\n",
    "\n",
    "### Fases Aplicadas:\n",
    "- **Fase 1-2:** Business Understanding & Data Understanding\n",
    "  - Análisis exploratorio completo (Celdas 3-24)\n",
    "  - Comprensión de objetivos de negocio\n",
    "  \n",
    "- **Fase 3:** Data Preparation\n",
    "  - Preprocesamiento y limpieza (Celdas 13-24)\n",
    "  - Creación de features temporales y RFM\n",
    "  \n",
    "- **Fase 4:** Modeling\n",
    "  - Implementación de 5 algoritmos (Celdas 27-40)\n",
    "  - User-Based CF, Item-Based CF, SVD, Híbrido\n",
    "  \n",
    "- **Fase 5:** Evaluation\n",
    "  - Métricas: Precision@10, Recall@10, F1-Score (Celdas 33-40)\n",
    "  - Comparación de modelos\n",
    "  \n",
    "- **Fase 6:** Deployment\n",
    "  - Estrategia de producción (Celdas 42-50)\n",
    "  - Monitorización y A/B Testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37e28bc",
   "metadata": {},
   "source": [
    "<a id=\"seccion-3-eda\"></a>\n",
    "# 📊 3. Análisis Exploratorio de Datos (EDA)\n",
    "\n",
    "---\n",
    "\n",
    "## Carga de Datasets\n",
    "\n",
    "En esta sección cargaremos y exploraremos los datos de transacciones y clientes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68219f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "data_transactions = pd.read_csv(\"dataset_sample_1.csv\")\n",
    "data_customers = pd.read_csv(\"dataset_sample_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ef741f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbaa6672",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_customers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0e8470",
   "metadata": {},
   "source": [
    "# Overview Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1a24a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check a overview of the dataset\n",
    "def overview(data):\n",
    "    print(\"=\".center(50,\"=\"))\n",
    "\n",
    "    # Print the shape of the dataset to see how many rows and columns it has.\n",
    "    print(f\"\\nOverview\")\n",
    "    print(f\"Shape: {data.shape}\")\n",
    "    print(f\"Memory Usage: {data.memory_usage().sum()/1024/1024:.2f} MB\")\n",
    "    print(\"=\".center(50,\"=\"))\n",
    "\n",
    "    # Display Index, Columns, and Data Types\n",
    "    print(\"Information about the features:\")\n",
    "    print(data.info())\n",
    "    print(\"=\".center(50,\"=\"))\n",
    "    dtype_counts = data.dtypes.value_counts()\n",
    "    for dtype, count in dtype_counts.items():\n",
    "        print(f\"{str(dtype):<20}: {count} columns\")\n",
    "    print(\"=\".center(50,\"=\"))\n",
    "\n",
    "    # Display summary statistics\n",
    "    print(\"Basic statistics check:\")\n",
    "    print(data.describe())\n",
    "    print(\"=\".center(50,\"=\"))\n",
    "\n",
    "    # I always run this part to understand the unique values in each column.\n",
    "    # It helps me get a sense of the data, especially which features are categorical or have low variability.\n",
    "    print(\"Checking the number of unique values:\")\n",
    "    unique_counts = {}\n",
    "    for column in data.columns:\n",
    "        unique_counts[column] = data[column].nunique()\n",
    "    unique_data = pd.DataFrame(unique_counts, index=[\"Unique Count\"]).transpose()\n",
    "    print(unique_data)\n",
    "    print(\"=\".center(50, \"=\"))\n",
    "\n",
    "    # Check for Missing Values\n",
    "    print(\"Check for missing values:\")\n",
    "    missing_values = data.isnull().sum()\n",
    "    missing_pct = (missing_values / len(data)) * 100\n",
    "    missing_data = pd.DataFrame({\n",
    "        'Missing Values': missing_values,\n",
    "        'Percentage (%)': missing_pct.round(2)\n",
    "    })\n",
    "    print(missing_data[missing_data['Missing Values'] > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a6b684",
   "metadata": {},
   "source": [
    "## Overview Data Transactiones (dataset_sample1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9b3f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "overview(data_transactions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8cbb1d",
   "metadata": {},
   "source": [
    "## Overview Data Customers (dataset_sample2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f812344",
   "metadata": {},
   "outputs": [],
   "source": [
    "overview(data_customers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67639098",
   "metadata": {},
   "source": [
    "<a id=\"seccion-4-preprocesamiento\"></a>\n",
    "# 🔧 4. Preprocesamiento y Preparación de Datos\n",
    "\n",
    "---\n",
    "\n",
    "## Limpieza y Transformación\n",
    "\n",
    "Aplicaremos técnicas de limpieza, manejo de valores nulos e ingeniería de características.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9f818e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpieza y preprocesamiento mejorado\n",
    "def preprocess_data(transactions_df, customers_df):\n",
    "    # Copiar los datos para no modificar los originales\n",
    "    transactions = transactions_df.copy()\n",
    "    customers = customers_df.copy()\n",
    "    \n",
    "    print(\"INICIANDO PREPROCESAMIENTO\")\n",
    "    \n",
    "    # 1. Manejo de valores nulos en transacciones\n",
    "    print(\"Manejo de valores nulos en transacciones...\")\n",
    "    transactions['CATEGORIA'] = transactions['CATEGORIA'].fillna('DESCONOCIDA')\n",
    "    \n",
    "    # 2. Manejo de valores nulos en clientes\n",
    "    print(\"Manejo de valores nulos en clientes...\")\n",
    "    customers['DEPARTAMENTO'] = customers['DEPARTAMENTO'].fillna('NO_ESPECIFICADO')\n",
    "    customers['CIUDAD'] = customers['CIUDAD'].fillna('NO_ESPECIFICADO')\n",
    "    # U para Unknown\n",
    "    customers['GENERO_DIM_CLIENTE'] = customers['GENERO_DIM_CLIENTE'].fillna('U')  \n",
    "    \n",
    "    # Convertir fecha de nacimiento y calcular edad\n",
    "    customers['FECHANACIMIENTO_DIM_CLIENTE'] = pd.to_datetime(\n",
    "        customers['FECHANACIMIENTO_DIM_CLIENTE'], errors='coerce'\n",
    "    )\n",
    "    \n",
    "    # Calcular edad\n",
    "    current_year = datetime.now().year\n",
    "    customers['EDAD'] = current_year - customers['FECHANACIMIENTO_DIM_CLIENTE'].dt.year\n",
    "    customers['EDAD'] = customers['EDAD'].fillna(customers['EDAD'].median())\n",
    "    \n",
    "    # Crear grupos de edad\n",
    "    def age_group(age):\n",
    "        if age <= 25: return '18-25'\n",
    "        elif age <= 35: return '26-35'\n",
    "        elif age <= 45: return '36-45'\n",
    "        elif age <= 55: return '46-55'\n",
    "        else: return '55+'\n",
    "    \n",
    "    customers['GRUPO_EDAD'] = customers['EDAD'].apply(age_group)\n",
    "    \n",
    "    # 3. Ingeniería de características temporales para transacciones\n",
    "    print(\"Ingeniería de características temporales...\")\n",
    "    transactions['FECHA_SOLUCION'] = pd.to_datetime(transactions['FECHA_SOLUCION'])\n",
    "    transactions['MES'] = transactions['FECHA_SOLUCION'].dt.month\n",
    "    transactions['DIA_SEMANA'] = transactions['FECHA_SOLUCION'].dt.day_name()\n",
    "    transactions['ES_FIN_DE_SEMANA'] = transactions['FECHA_SOLUCION'].dt.dayofweek >= 5\n",
    "    \n",
    "    # 4. Crear variable de valor por unidad\n",
    "    transactions['VALOR_POR_UNIDAD'] = transactions['VENTA_BRUTA_CON_IVA'] / transactions['UNIDADES_BRUTAS']\n",
    "    \n",
    "    print(\"Preprocesamiento completado ✓\")\n",
    "    return transactions, customers\n",
    "\n",
    "# Aplicar preprocesamiento\n",
    "data_transactions_clean, data_customers_clean = preprocess_data(data_transactions, data_customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf73b353",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_customers_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e44a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transactions_clean.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d32b94",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972507c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estadísticas para transacciones\n",
    "print(\"ESTADÍSTICAS DESCRIPTIVAS - TRANSACCIONES \")\n",
    "print(data_transactions_clean[['UNIDADES_BRUTAS', 'VENTA_BRUTA_CON_IVA']].describe())\n",
    "\n",
    "# Análisis de categorías de productos\n",
    "print(\"\\n DISTRIBUCIÓN DE CATEGORÍAS \")\n",
    "category_stats = data_transactions_clean['CATEGORIA'].value_counts()\n",
    "print(f\"Número de categorías únicas: {len(category_stats)}\")\n",
    "print(\"Top 10 categorías:\")\n",
    "print(category_stats.head(10))\n",
    "\n",
    "# Análisis temporal\n",
    "data_transactions_clean['FECHA_SOLUCION'] = pd.to_datetime(data_transactions_clean['FECHA_SOLUCION'])\n",
    "print(f\"\\nRango temporal de transacciones:\")\n",
    "print(f\"Fecha más antigua: {data_transactions_clean['FECHA_SOLUCION'].min()}\")\n",
    "print(f\"Fecha más reciente: {data_transactions_clean['FECHA_SOLUCION'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21cd4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis del comportamiento de clientes\n",
    "def customer_behavior_analysis(transactions, customers):\n",
    "    print(\"ANÁLISIS DE COMPORTAMIENTO DE CLIENTES\")\n",
    "    \n",
    "    # Unir datos de transacciones con información de clientes\n",
    "    merged_data = transactions.merge(\n",
    "        customers[['UUID_CLIENTE_CONSUMIDOR', 'GENERO_DIM_CLIENTE', 'EDAD', 'GRUPO_EDAD', 'DEPARTAMENTO']],\n",
    "        on='UUID_CLIENTE_CONSUMIDOR',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # 1. Comportamiento por grupo de edad\n",
    "    print(\"\\n1. COMPORTAMIENTO POR GRUPO DE EDAD:\")\n",
    "    age_behavior = merged_data.groupby('GRUPO_EDAD').agg({\n",
    "        'VENTA_BRUTA_CON_IVA': ['mean', 'sum', 'count'],\n",
    "        'UNIDADES_BRUTAS': 'mean',\n",
    "        'UUID_CLIENTE_CONSUMIDOR': 'nunique'\n",
    "    }).round(2)\n",
    "    print(age_behavior)\n",
    "    \n",
    "    # 2. Comportamiento por género\n",
    "    print(\"\\n2. COMPORTAMIENTO POR GÉNERO:\")\n",
    "    gender_behavior = merged_data.groupby('GENERO_DIM_CLIENTE').agg({\n",
    "        'VENTA_BRUTA_CON_IVA': ['mean', 'sum'],\n",
    "        'UNIDADES_BRUTAS': 'mean',\n",
    "        'UUID_CLIENTE_CONSUMIDOR': 'nunique'\n",
    "    }).round(2)\n",
    "    print(gender_behavior)\n",
    "    \n",
    "    # 3. Frecuencia de compra por cliente\n",
    "    print(\"\\n3. FRECUENCIA DE COMPRA POR CLIENTE:\")\n",
    "    customer_frequency = transactions.groupby('UUID_CLIENTE_CONSUMIDOR').agg({\n",
    "        'PEDIDO': 'count',\n",
    "        'VENTA_BRUTA_CON_IVA': 'sum',\n",
    "        'UNIDADES_BRUTAS': 'sum'\n",
    "    }).rename(columns={'PEDIDO': 'FRECUENCIA_COMPRA'})\n",
    "    \n",
    "    print(f\"Clientes con una sola compra: {(customer_frequency['FRECUENCIA_COMPRA'] == 1).sum()}\")\n",
    "    print(f\"Clientes recurrentes (2+ compras): {(customer_frequency['FRECUENCIA_COMPRA'] > 1).sum()}\")\n",
    "    print(f\"Frecuencia promedio de compra: {customer_frequency['FRECUENCIA_COMPRA'].mean():.2f}\")\n",
    "    \n",
    "    return merged_data, customer_frequency\n",
    "\n",
    "# Ejecutar análisis de comportamiento\n",
    "merged_data, customer_frequency = customer_behavior_analysis(data_transactions_clean, data_customers_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bc9d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaciones más completas\n",
    "def create_enhanced_visualizations(transactions, customers, customer_freq):\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(20, 18))\n",
    "    fig.suptitle('Análisis Exploratorio de Datos', fontsize=20, fontweight='bold', y=1)\n",
    "    \n",
    "    # 1. Distribución de frecuencia de compra (mejorada)\n",
    "    freq_counts = customer_freq['FRECUENCIA_COMPRA'].value_counts().sort_index().head(15)\n",
    "    axes[0, 0].bar(freq_counts.index, freq_counts.values, color='lightseagreen', alpha=0.7)\n",
    "    axes[0, 0].set_title('Distribución de Frecuencia de Compra por Cliente', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Número de Compras')\n",
    "    axes[0, 0].set_ylabel('Número de Clientes')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Valor de cliente (RFM simplificado)\n",
    "    top_customers = customer_freq.nlargest(10, 'VENTA_BRUTA_CON_IVA')\n",
    "    customer_value = top_customers['VENTA_BRUTA_CON_IVA']\n",
    "\n",
    "    # Crear etiquetas más informativas\n",
    "    labels = []\n",
    "    for i, (customer_id, row) in enumerate(top_customers.iterrows()):\n",
    "        short_id = customer_id[-8:]  # Últimos 8 caracteres del UUID\n",
    "        freq = row['FRECUENCIA_COMPRA']\n",
    "        labels.append(f'Cliente {i+1}\\n({short_id}...)\\n{freq} compras')\n",
    "\n",
    "    axes[0, 1].barh(range(len(customer_value)), customer_value.values, color='coral', alpha=0.8)\n",
    "    axes[0, 1].set_yticks(range(len(customer_value)))\n",
    "    axes[0, 1].set_yticklabels(labels, fontsize=9)\n",
    "    axes[0, 1].set_title('Top 10 Clientes por Valor Total', fontsize=12, fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Valor Total de Compras (COP)')\n",
    "\n",
    "    # Añadir valores en las barras\n",
    "    for i, v in enumerate(customer_value.values):\n",
    "        axes[0, 1].text(v + v*0.01, i, f'${v:,.0f}', \n",
    "                    va='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "    # Añadir cuadro informativo\n",
    "    total_top = customer_value.sum()\n",
    "    avg_value = customer_value.mean()\n",
    "    axes[0, 1].text(0.02, 0.98, f'Total: ${total_top:,.0f}\\nPromedio: ${avg_value:,.0f}', \n",
    "                transform=axes[0, 1].transAxes, verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # 3. Análisis de categorías por valor\n",
    "    category_value = transactions.groupby('CATEGORIA').agg({\n",
    "        'VENTA_BRUTA_CON_IVA': 'sum',\n",
    "        'UUID_CLIENTE_CONSUMIDOR': 'count'\n",
    "    }).nlargest(10, 'VENTA_BRUTA_CON_IVA')\n",
    "    \n",
    "    axes[1, 0].barh(range(len(category_value)), category_value['VENTA_BRUTA_CON_IVA'].values, color='goldenrod')\n",
    "    axes[1, 0].set_yticks(range(len(category_value)))\n",
    "    axes[1, 0].set_yticklabels(category_value.index, fontsize=10)\n",
    "    axes[1, 0].set_title('Top 10 Categorías por Valor Total de Ventas', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Valor Total de Ventas')\n",
    "        \n",
    "    # 5. Comportamiento por día de la semana\n",
    "    weekday_sales = transactions.groupby('DIA_SEMANA')['VENTA_BRUTA_CON_IVA'].sum()\n",
    "    weekday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    weekday_sales = weekday_sales.reindex(weekday_order)\n",
    "    \n",
    "    axes[1, 1].bar(range(len(weekday_sales)), weekday_sales.values, color='lightcoral')\n",
    "    axes[1, 1].set_xticks(range(len(weekday_sales)))\n",
    "    axes[1, 1].set_xticklabels(['Lun', 'Mar', 'Mié', 'Jue', 'Vie', 'Sáb', 'Dom'])\n",
    "    axes[1, 1].set_title('Ventas Totales por Día de la Semana', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].set_ylabel('Ventas Totales')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Distribución geográfica de ventas\n",
    "    geo_sales = merged_data.groupby('DEPARTAMENTO')['VENTA_BRUTA_CON_IVA'].sum().nlargest(10)\n",
    "    axes[2, 0].barh(range(len(geo_sales)), geo_sales.values, color='lightgreen')\n",
    "    axes[2, 0].set_yticks(range(len(geo_sales)))\n",
    "    axes[2, 0].set_yticklabels(geo_sales.index, fontsize=10)\n",
    "    axes[2, 0].set_title('Top 10 Departamentos por Ventas Totales', fontsize=14, fontweight='bold')\n",
    "    axes[2, 0].set_xlabel('Ventas Totales')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Crear visualizaciones mejoradas\n",
    "create_enhanced_visualizations(data_transactions_clean, data_customers_clean, customer_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a6d5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Distribución de valor por unidad\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Eliminar outliers extremos para mejor visualización\n",
    "Q1 = data_transactions_clean['VALOR_POR_UNIDAD'].quantile(0.01)\n",
    "Q3 = data_transactions_clean['VALOR_POR_UNIDAD'].quantile(0.99)\n",
    "filtered_values = data_transactions_clean[(data_transactions_clean['VALOR_POR_UNIDAD'] >= Q1) & \n",
    "                            (data_transactions_clean['VALOR_POR_UNIDAD'] <= Q3)]['VALOR_POR_UNIDAD']\n",
    "\n",
    "plt.hist(filtered_values, bins=50, color='mediumpurple', alpha=0.7, edgecolor='black', density=True)\n",
    "plt.title('Distribución de Valor por Unidad (sin outliers extremos)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Valor por Unidad (COP)')\n",
    "plt.ylabel('Densidad')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Añadir líneas de percentiles\n",
    "percentiles = [25, 50, 75, 90]\n",
    "colors = ['red', 'green', 'blue', 'orange']\n",
    "for p, color in zip(percentiles, colors):\n",
    "    value = data_transactions_clean['VALOR_POR_UNIDAD'].quantile(p/100)\n",
    "    plt.axvline(value, color=color, linestyle='--', alpha=0.8, \n",
    "                label=f'P{p}: ${value:,.0f} COP')\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bed5c24",
   "metadata": {},
   "source": [
    "# Preparación para el Sistema de Recomendación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265256dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar datos para el modelo de recomendación\n",
    "def prepare_recommendation_data(transactions):\n",
    "    print(\"PREPARANDO DATOS PARA RECOMENDACIÓN\")\n",
    "    \n",
    "    # Crear matriz de interacciones usuario-producto\n",
    "    user_item_interactions = transactions.groupby(['UUID_CLIENTE_CONSUMIDOR', 'COD_PRODUCTO']).agg({\n",
    "        # Total de unidades compradas\n",
    "        'UNIDADES_BRUTAS': 'sum',  \n",
    "        # Valor total gastado\n",
    "        'VENTA_BRUTA_CON_IVA': 'sum',  \n",
    "        # Frecuencia de compra\n",
    "        'PEDIDO': 'count'  \n",
    "    }).reset_index()\n",
    "    \n",
    "    user_item_interactions.rename(columns={'PEDIDO': 'FRECUENCIA_COMPRA'}, inplace=True)\n",
    "    \n",
    "    # Calcular estadísticas generales\n",
    "    n_users = user_item_interactions['UUID_CLIENTE_CONSUMIDOR'].nunique()\n",
    "    n_items = user_item_interactions['COD_PRODUCTO'].nunique()\n",
    "    n_interactions = len(user_item_interactions)\n",
    "    \n",
    "    print(f\"Estadísticas del Dataset:\")\n",
    "    print(f\"- Usuarios únicos: {n_users:,}\")\n",
    "    print(f\"- Productos únicos: {n_items:,}\")\n",
    "    print(f\"- Interacciones totales: {n_interactions:,}\")\n",
    "    print(f\"- Densidad de la matriz: {(n_interactions / (n_users * n_items)):.3f}\")\n",
    "    \n",
    "    # Analizar la distribución de interacciones por usuario\n",
    "    interactions_per_user = user_item_interactions.groupby('UUID_CLIENTE_CONSUMIDOR').size()\n",
    "    print(f\"\\nDistribución de interacciones por usuario:\")\n",
    "    print(interactions_per_user.describe())\n",
    "    \n",
    "    return user_item_interactions\n",
    "\n",
    "# Datos para recomendación\n",
    "user_item_data = prepare_recommendation_data(data_transactions_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd80bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis para manejar el problema cold start\n",
    "def cold_start_analysis(transactions, user_item_data):\n",
    "    print(\"ANÁLISIS PARA PROBLEMA COLD START\")\n",
    "    \n",
    "    # Productos más populares (más comprados)\n",
    "    popular_products = transactions.groupby('COD_PRODUCTO').agg({\n",
    "        'UUID_CLIENTE_CONSUMIDOR': 'count',\n",
    "        'VENTA_BRUTA_CON_IVA': 'sum',\n",
    "        'CATEGORIA': 'first'\n",
    "    }).rename(columns={'UUID_CLIENTE_CONSUMIDOR': 'NUM_COMPRAS'})\n",
    "    \n",
    "    popular_products = popular_products.sort_values('NUM_COMPRAS', ascending=False)\n",
    "    \n",
    "    print(\"Top 10 productos más populares:\")\n",
    "    for i, (product_id, row) in enumerate(popular_products.head(10).iterrows(), 1):\n",
    "        print(f\"{i}. Producto {product_id} - {row['CATEGORIA']} - {row['NUM_COMPRAS']} compras\")\n",
    "    \n",
    "    # Categorías más populares\n",
    "    popular_categories = transactions['CATEGORIA'].value_counts().head(10)\n",
    "    print(f\"\\nTop 10 categorías más populares:\")\n",
    "    for i, (category, count) in enumerate(popular_categories.items(), 1):\n",
    "        print(f\"{i}. {category} - {count} transacciones\")\n",
    "    \n",
    "    return popular_products, popular_categories\n",
    "\n",
    "# Ejecutar análisis cold start\n",
    "popular_products, popular_categories = cold_start_analysis(data_transactions_clean, user_item_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc473361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un resumen ejecutivo del análisis\n",
    "def create_executive_summary(transactions, customers, user_item_data):\n",
    "    print(\"=\" * 80)\n",
    "    print(\"RESUMEN EJECUTIVO - ANÁLISIS EXPLORATORIO DE DATOS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Métricas clave\n",
    "    total_transactions = len(transactions)\n",
    "    total_customers = transactions['UUID_CLIENTE_CONSUMIDOR'].nunique()\n",
    "    total_products = transactions['COD_PRODUCTO'].nunique()\n",
    "    total_revenue = transactions['VENTA_BRUTA_CON_IVA'].sum()\n",
    "    avg_transaction_value = transactions['VENTA_BRUTA_CON_IVA'].mean()\n",
    "    \n",
    "    print(f\"\\n📊 MÉTRICAS PRINCIPALES:\")\n",
    "    print(f\"- Transacciones totales: {total_transactions:,}\")\n",
    "    print(f\"- Clientes únicos: {total_customers:,}\")\n",
    "    print(f\"- Productos únicos: {total_products:,}\")\n",
    "    print(f\"- Ingresos totales: ${total_revenue:,.0f} COP\")\n",
    "    print(f\"- Valor promedio por transacción: ${avg_transaction_value:,.0f} COP\")\n",
    "    \n",
    "    # Comportamiento del cliente\n",
    "    customer_freq = user_item_data.groupby('UUID_CLIENTE_CONSUMIDOR').size()\n",
    "    repeat_customers = (customer_freq > 1).sum()\n",
    "    repeat_rate = (repeat_customers / total_customers) * 100\n",
    "    \n",
    "    print(f\"\\n👥 COMPORTAMIENTO DEL CLIENTE:\")\n",
    "    print(f\"* Tasa de clientes recurrentes: {repeat_rate:.1f}%\")\n",
    "    print(f\"* Frecuencia promedio de compra: {customer_freq.mean():.2f}\")\n",
    "    print(f\"* Clientes con una sola compra: {(customer_freq == 1).sum():,}\")\n",
    "    \n",
    "    # Análisis de productos\n",
    "    top_category = transactions['CATEGORIA'].value_counts().index[0]\n",
    "    top_category_count = transactions['CATEGORIA'].value_counts().iloc[0]\n",
    "    \n",
    "    print(f\"\\n📦 ANÁLISIS DE PRODUCTOS:\")\n",
    "    print(f\"+ Categoría más popular: {top_category} ({top_category_count} transacciones)\")\n",
    "    print(f\"+ Total de categorías: {transactions['CATEGORIA'].nunique()}\")\n",
    "    print(f\"+ Densidad user-item: {(len(user_item_data) / (total_customers * total_products)):.6f}\")    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# Generar resumen ejecutivo\n",
    "create_executive_summary(data_transactions_clean, data_customers_clean, user_item_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfde078",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708b448c",
   "metadata": {},
   "source": [
    "<a id=\"seccion-5-sistema-recomendacion\"></a>\n",
    "# 🤖 5. Sistema de Recomendación\n",
    "\n",
    "---\n",
    "\n",
    "## Implementación de Algoritmos\n",
    "\n",
    "Se implementarán 5 algoritmos de recomendación:\n",
    "1. **Popularidad** (Baseline)\n",
    "2. **User-Based Collaborative Filtering**\n",
    "3. **Item-Based Collaborative Filtering**\n",
    "4. **SVD (Matrix Factorization)**\n",
    "5. **Modelo Híbrido** (Combinación de algoritmos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf0add4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear matriz de utilidad usuario-item\n",
    "def create_user_item_matrix(transactions_df):\n",
    "    print(\"Creando matriz usuario-item...\")\n",
    "    \n",
    "    # Calcular frecuencia de compra y valor total por usuario-producto\n",
    "    user_item_df = transactions_df.groupby(['UUID_CLIENTE_CONSUMIDOR', 'COD_PRODUCTO']).agg({\n",
    "        'UNIDADES_BRUTAS': 'sum',\n",
    "        'VENTA_BRUTA_CON_IVA': 'sum',\n",
    "        'PEDIDO': 'count',\n",
    "        'CATEGORIA': 'first'\n",
    "    }).reset_index()\n",
    "    \n",
    "    user_item_df.rename(columns={'PEDIDO': 'FRECUENCIA_COMPRA'}, inplace=True)\n",
    "    \n",
    "    # Crear mapeos\n",
    "    user_ids = user_item_df['UUID_CLIENTE_CONSUMIDOR'].unique()\n",
    "    item_ids = user_item_df['COD_PRODUCTO'].unique()\n",
    "    \n",
    "    user_to_idx = {user: idx for idx, user in enumerate(user_ids)}\n",
    "    idx_to_user = {idx: user for user, idx in user_to_idx.items()}\n",
    "    item_to_idx = {item: idx for idx, item in enumerate(item_ids)}\n",
    "    idx_to_item = {idx: item for item, idx in item_to_idx.items()}\n",
    "    \n",
    "    # Crear matriz sparse\n",
    "    rows = [user_to_idx[user] for user in user_item_df['UUID_CLIENTE_CONSUMIDOR']]\n",
    "    cols = [item_to_idx[item] for item in user_item_df['COD_PRODUCTO']]\n",
    "    # Usar frecuencia como medida de preferencia\n",
    "    data = user_item_df['FRECUENCIA_COMPRA'].values  \n",
    "    \n",
    "    utility_matrix = csr_matrix((data, (rows, cols)), \n",
    "                                shape=(len(user_ids), len(item_ids)))\n",
    "    \n",
    "    print(f\"Matriz creada: {utility_matrix.shape[0]} usuarios, {utility_matrix.shape[1]} productos\")\n",
    "    print(f\"Densidad: {(len(data) / (len(user_ids) * len(item_ids))):.6f}\")\n",
    "    \n",
    "    return utility_matrix, user_to_idx, idx_to_user, item_to_idx, idx_to_item, user_item_df\n",
    "\n",
    "# Crear matriz de utilidad\n",
    "utility_matrix, user_to_idx, idx_to_user, item_to_idx, idx_to_item, user_item_df = create_user_item_matrix(data_transactions_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201cbcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir datos en entrenamiento y prueba manteniendo la estructura de la matriz\n",
    "def split_train_test(utility_matrix, test_size=0.2, random_state=42):\n",
    "    print(\"Dividiendo datos en train/test...\")\n",
    "    \n",
    "    # Convertir a formato COO para manipulación\n",
    "    coo_matrix_data = utility_matrix.tocoo()\n",
    "    \n",
    "    # Obtener índices de las interacciones\n",
    "    indices = list(zip(coo_matrix_data.row, coo_matrix_data.col))\n",
    "    data = coo_matrix_data.data\n",
    "    \n",
    "    # Dividir índices\n",
    "    train_indices, test_indices = train_test_split(\n",
    "        range(len(indices)), test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Crear matrices train y test\n",
    "    train_rows = [indices[i][0] for i in train_indices]\n",
    "    train_cols = [indices[i][1] for i in train_indices]\n",
    "    train_data = [data[i] for i in train_indices]\n",
    "    \n",
    "    test_rows = [indices[i][0] for i in test_indices]\n",
    "    test_cols = [indices[i][1] for i in test_indices]\n",
    "    test_data = [data[i] for i in test_indices]\n",
    "    \n",
    "    train_matrix = csr_matrix((train_data, (train_rows, train_cols)), \n",
    "                                shape=utility_matrix.shape)\n",
    "    test_matrix = csr_matrix((test_data, (test_rows, test_cols)), \n",
    "                            shape=utility_matrix.shape)\n",
    "    \n",
    "    print(f\"Train: {len(train_data)} interacciones\")\n",
    "    print(f\"Test: {len(test_data)} interacciones\")\n",
    "    \n",
    "    return train_matrix, test_matrix, test_indices\n",
    "\n",
    "# Dividir datos\n",
    "train_matrix, test_matrix, test_indices = split_train_test(utility_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a99ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sistema de recomendación con múltiples algoritmos\n",
    "class RecommendationSystem:   \n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.metrics = {}\n",
    "        \n",
    "    # Modelo basado en popularidad\n",
    "    def popular_items_model(self, train_matrix, top_n=50):\n",
    "        print(\"Entrenando modelo de popularidad...\")\n",
    "        \n",
    "        # Calcular popularidad de items (suma de interacciones)\n",
    "        item_popularity = np.array(train_matrix.sum(axis=0)).flatten()\n",
    "        \n",
    "        # Obtener top N items más populares\n",
    "        popular_items = np.argsort(item_popularity)[::-1][:top_n]\n",
    "        \n",
    "        self.models['popularity'] = {\n",
    "            'type': 'popularity',\n",
    "            'popular_items': popular_items,\n",
    "            'item_scores': item_popularity\n",
    "        }\n",
    "        \n",
    "        return popular_items\n",
    "    \n",
    "    # Filtrado colaborativo basado en usuario\n",
    "    def user_based_cf(self, train_matrix, min_similar_users=5):\n",
    "        print(\"Entrenando modelo User-Based CF...\")\n",
    "        \n",
    "        # Calcular similitud entre usuarios\n",
    "        user_similarity = cosine_similarity(train_matrix)\n",
    "        \n",
    "        # Para cada usuario, encontrar usuarios similares\n",
    "        user_predictions = {}\n",
    "        for user_idx in range(train_matrix.shape[0]):\n",
    "            # Obtener usuarios similares (excluyendo al mismo usuario)\n",
    "            similar_users = np.argsort(user_similarity[user_idx])[::-1][1:min_similar_users+1]\n",
    "            user_predictions[user_idx] = similar_users\n",
    "        \n",
    "        self.models['user_based'] = {\n",
    "            'type': 'user_based',\n",
    "            'user_similarity': user_similarity,\n",
    "            'user_predictions': user_predictions\n",
    "        }\n",
    "        \n",
    "        return user_similarity\n",
    "    \n",
    "    # Filtrado colaborativo basado en ítem\n",
    "    def item_based_cf(self, train_matrix, min_similar_items=10):\n",
    "        print(\"Entrenando modelo Item-Based CF...\")\n",
    "        \n",
    "        # Calcular similitud entre ítems\n",
    "        item_similarity = cosine_similarity(train_matrix.T)\n",
    "        \n",
    "        self.models['item_based'] = {\n",
    "            'type': 'item_based',\n",
    "            'item_similarity': item_similarity\n",
    "        }\n",
    "        \n",
    "        return item_similarity\n",
    "    \n",
    "    # Factorización de matrices con SVD\n",
    "    def matrix_factorization(self, train_matrix, n_components=50):\n",
    "        print(\"Entrenando modelo de Factorización de Matrices...\")\n",
    "        \n",
    "        svd = TruncatedSVD(n_components=n_components, random_state=42)\n",
    "        user_factors = svd.fit_transform(train_matrix)\n",
    "        item_factors = svd.components_.T\n",
    "        \n",
    "        self.models['svd'] = {\n",
    "            'type': 'matrix_factorization',\n",
    "            'model': svd,\n",
    "            'user_factors': user_factors,\n",
    "            'item_factors': item_factors,\n",
    "            'explained_variance': svd.explained_variance_ratio_.sum()\n",
    "        }\n",
    "        \n",
    "        print(f\"Varianza explicada: {svd.explained_variance_ratio_.sum():.4f}\")\n",
    "        \n",
    "        return user_factors, item_factors\n",
    "    \n",
    "    # Modelo híbrido que combina múltiples enfoques\n",
    "    def hybrid_model(self, train_matrix, weights=None):\n",
    "        print(\"Entrenando modelo híbrido...\")\n",
    "        \n",
    "        if weights is None:\n",
    "            weights = {'popularity': 0.2, 'item_based': 0.4, 'svd': 0.4}\n",
    "        \n",
    "        # Entrenar modelos componentes\n",
    "        self.popular_items_model(train_matrix)\n",
    "        self.item_based_cf(train_matrix)\n",
    "        self.matrix_factorization(train_matrix)\n",
    "        \n",
    "        self.models['hybrid'] = {\n",
    "            'type': 'hybrid',\n",
    "            'weights': weights\n",
    "        }\n",
    "        \n",
    "        return weights\n",
    "\n",
    "# Entrenar todos los modelos\n",
    "recommender = RecommendationSystem()\n",
    "\n",
    "# Modelo de popularidad\n",
    "popular_items = recommender.popular_items_model(train_matrix)\n",
    "\n",
    "# Modelo user-based\n",
    "user_similarity = recommender.user_based_cf(train_matrix)\n",
    "\n",
    "# Modelo item-based\n",
    "item_similarity = recommender.item_based_cf(train_matrix)\n",
    "\n",
    "# Modelo de factorización de matrices\n",
    "user_factors, item_factors = recommender.matrix_factorization(train_matrix, n_components=50)\n",
    "\n",
    "# Modelo híbrido\n",
    "hybrid_weights = recommender.hybrid_model(train_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16369098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar recomendaciones para un usuario específico\n",
    "def generate_recommendations(recommender, user_id, user_to_idx, idx_to_item, \n",
    "                            item_to_idx, user_item_df, top_n=10, method='hybrid'):\n",
    "    if user_id not in user_to_idx:\n",
    "        return get_popular_recommendations(recommender, idx_to_item, user_item_df, top_n)\n",
    "    \n",
    "    user_idx = user_to_idx[user_id]\n",
    "    \n",
    "    if method == 'popularity':\n",
    "        return _get_popular_recommendations(recommender, idx_to_item, user_item_df, top_n)\n",
    "    elif method == 'user_based':\n",
    "        return _get_user_based_recommendations(recommender, user_idx, idx_to_item, user_item_df, top_n)\n",
    "    elif method == 'item_based':\n",
    "        return _get_item_based_recommendations(recommender, user_idx, train_matrix, \n",
    "                                                idx_to_item, user_item_df, top_n)\n",
    "    elif method == 'svd':\n",
    "        return _get_svd_recommendations(recommender, user_idx, idx_to_item, user_item_df, top_n)\n",
    "    elif method == 'hybrid':\n",
    "        return _get_hybrid_recommendations(recommender, user_idx, train_matrix, \n",
    "                                            idx_to_item, user_item_df, top_n)\n",
    "    else:\n",
    "        raise ValueError(f\"Método {method} no soportado\")\n",
    "\n",
    "# Recomendaciones basadas en popularidad\n",
    "def _get_popular_recommendations(recommender, idx_to_item, user_item_df, top_n):\n",
    "    popular_items = recommender.models['popularity']['popular_items']\n",
    "    recommendations = []\n",
    "    \n",
    "    for item_idx in popular_items[:top_n]:\n",
    "        product_id = idx_to_item[item_idx]\n",
    "        product_info = user_item_df[user_item_df['COD_PRODUCTO'] == product_id].iloc[0]\n",
    "        recommendations.append({\n",
    "            'COD_PRODUCTO': product_id,\n",
    "            'CATEGORIA': product_info['CATEGORIA'],\n",
    "            'SCORE': recommender.models['popularity']['item_scores'][item_idx],\n",
    "            'METHOD': 'popularity'\n",
    "        })\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Recomendaciones user-based CF\n",
    "def _get_user_based_recommendations(recommender, user_idx, idx_to_item, user_item_df, top_n):\n",
    "    similar_users = recommender.models['user_based']['user_predictions'][user_idx]\n",
    "    user_similarity = recommender.models['user_based']['user_similarity']\n",
    "    \n",
    "    # Calcular scores basados en usuarios similares\n",
    "    scores = np.zeros(train_matrix.shape[1])\n",
    "    for sim_user in similar_users:\n",
    "        similarity_weight = user_similarity[user_idx, sim_user]\n",
    "        scores += similarity_weight * train_matrix[sim_user].toarray().flatten()\n",
    "    \n",
    "    return _get_recommendations_from_scores(scores, user_idx, idx_to_item, user_item_df, top_n, 'user_based')\n",
    "\n",
    "# Recomendaciones item-based CF\n",
    "def _get_item_based_recommendations(recommender, user_idx, train_matrix, idx_to_item, user_item_df, top_n):\n",
    "    item_similarity = recommender.models['item_based']['item_similarity']\n",
    "    user_interactions = train_matrix[user_idx].toarray().flatten()\n",
    "    \n",
    "    # Calcular scores basados en similitud de ítems\n",
    "    scores = user_interactions @ item_similarity\n",
    "    \n",
    "    return _get_recommendations_from_scores(scores, user_idx, idx_to_item, user_item_df, top_n, 'item_based')\n",
    "\n",
    "# Recomendaciones con factorización de matrices\n",
    "def _get_svd_recommendations(recommender, user_idx, idx_to_item, user_item_df, top_n):\n",
    "    user_vector = recommender.models['svd']['user_factors'][user_idx]\n",
    "    item_factors = recommender.models['svd']['item_factors']\n",
    "    \n",
    "    scores = user_vector @ item_factors.T\n",
    "    \n",
    "    return _get_recommendations_from_scores(scores, user_idx, idx_to_item, user_item_df, top_n, 'svd')\n",
    "\n",
    "# Recomendaciones híbridas\n",
    "def _get_hybrid_recommendations(recommender, user_idx, train_matrix, idx_to_item, user_item_df, top_n):\n",
    "    weights = recommender.models['hybrid']['weights']\n",
    "    \n",
    "    # Obtener scores de cada modelo\n",
    "    item_based_scores = _get_item_based_scores(recommender, user_idx, train_matrix)\n",
    "    svd_scores = _get_svd_scores(recommender, user_idx)\n",
    "    popularity_scores = recommender.models['popularity']['item_scores']\n",
    "    \n",
    "    # Normalizar scores\n",
    "    item_based_scores = (item_based_scores - item_based_scores.min()) / (item_based_scores.max() - item_based_scores.min() + 1e-8)\n",
    "    svd_scores = (svd_scores - svd_scores.min()) / (svd_scores.max() - svd_scores.min() + 1e-8)\n",
    "    popularity_scores = (popularity_scores - popularity_scores.min()) / (popularity_scores.max() - popularity_scores.min() + 1e-8)\n",
    "    \n",
    "    # Combinar scores\n",
    "    hybrid_scores = (weights['item_based'] * item_based_scores + \n",
    "                    weights['svd'] * svd_scores + \n",
    "                    weights['popularity'] * popularity_scores)\n",
    "    \n",
    "    return _get_recommendations_from_scores(hybrid_scores, user_idx, idx_to_item, user_item_df, top_n, 'hybrid')\n",
    "\n",
    "# Obtener scores para modelo item-based\n",
    "def _get_item_based_scores(recommender, user_idx, train_matrix):\n",
    "    item_similarity = recommender.models['item_based']['item_similarity']\n",
    "    user_interactions = train_matrix[user_idx].toarray().flatten()\n",
    "    return user_interactions @ item_similarity\n",
    "\n",
    "# Obtener scores para modelo SVD\n",
    "def _get_svd_scores(recommender, user_idx):\n",
    "    user_vector = recommender.models['svd']['user_factors'][user_idx]\n",
    "    item_factors = recommender.models['svd']['item_factors']\n",
    "    return user_vector @ item_factors.T\n",
    "\n",
    "# Convertir scores en recomendaciones\n",
    "def _get_recommendations_from_scores(scores, user_idx, idx_to_item, user_item_df, top_n, method):\n",
    "    # Excluir ítems ya comprados\n",
    "    purchased_items = train_matrix[user_idx].indices\n",
    "    scores[purchased_items] = -np.inf\n",
    "    \n",
    "    # Obtener top N recomendaciones\n",
    "    top_item_indices = np.argsort(scores)[::-1][:top_n]\n",
    "    recommendations = []\n",
    "    \n",
    "    for item_idx in top_item_indices:\n",
    "        if scores[item_idx] > -np.inf:\n",
    "            product_id = idx_to_item[item_idx]\n",
    "            product_info = user_item_df[user_item_df['COD_PRODUCTO'] == product_id].iloc[0]\n",
    "            recommendations.append({\n",
    "                'COD_PRODUCTO': product_id,\n",
    "                'CATEGORIA': product_info['CATEGORIA'],\n",
    "                'SCORE': scores[item_idx],\n",
    "                'METHOD': method\n",
    "            })\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Recomendaciones populares para usuarios nuevos\n",
    "def get_popular_recommendations(recommender, idx_to_item, user_item_df, top_n):\n",
    "    return _get_popular_recommendations(recommender, idx_to_item, user_item_df, top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbeaa894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar resultados de evaluación\n",
    "def plot_evaluation_results(evaluation_results):\n",
    "    methods = list(evaluation_results.keys())\n",
    "    precision_scores = [evaluation_results[method]['precision@k'] for method in methods]\n",
    "    recall_scores = [evaluation_results[method]['recall@k'] for method in methods]\n",
    "    f1_scores = [evaluation_results[method]['f1_score'] for method in methods]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    # Precision\n",
    "    bars1 = axes[0].bar(methods, precision_scores, color='skyblue', alpha=0.8)\n",
    "    axes[0].set_title('Precision@10 por Modelo', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_ylabel('Precision')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Recall\n",
    "    bars2 = axes[1].bar(methods, recall_scores, color='lightcoral', alpha=0.8)\n",
    "    axes[1].set_title('Recall@10 por Modelo', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_ylabel('Recall')\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # F1-Score\n",
    "    bars3 = axes[2].bar(methods, f1_scores, color='lightgreen', alpha=0.8)\n",
    "    axes[2].set_title('F1-Score por Modelo', fontsize=14, fontweight='bold')\n",
    "    axes[2].set_ylabel('F1-Score')\n",
    "    axes[2].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Añadir valores en las barras\n",
    "    for bars, ax in zip([bars1, bars2, bars3], axes):\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height + 0.001,\n",
    "                    f'{height:.4f}', ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746ee395",
   "metadata": {},
   "source": [
    "<a id=\"seccion-6-evaluacion\"></a>\n",
    "# 📈 6. Evaluación de Modelos\n",
    "\n",
    "---\n",
    "\n",
    "## Métricas y Comparación de Performance\n",
    "\n",
    "Evaluaremos todos los modelos usando:\n",
    "- **Precision@10**: Precisión de las recomendaciones\n",
    "- **Recall@10**: Cobertura de items relevantes\n",
    "- **F1-Score**: Balance entre precisión y recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dea36c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models_fixed(recommender, test_matrix, user_to_idx, idx_to_item, user_item_df, top_n=10):\n",
    "    print(\"Evaluando modelos (versión corregida)...\")\n",
    "    \n",
    "    methods = ['popularity', 'item_based', 'svd', 'hybrid']\n",
    "    results = {}\n",
    "    \n",
    "    for method in methods:\n",
    "        print(f\"Evaluando {method}...\")\n",
    "        precision_scores = []\n",
    "        recall_scores = []\n",
    "        \n",
    "        # Evaluar en una muestra más pequeña para debug\n",
    "        test_users = list(user_to_idx.keys())[:100]\n",
    "        users_evaluated = 0\n",
    "        \n",
    "        for user_id in test_users:\n",
    "            try:\n",
    "                user_idx = user_to_idx[user_id]\n",
    "                \n",
    "                # Ítems reales en test\n",
    "                actual_items = set(test_matrix[user_idx].indices)\n",
    "                \n",
    "                if len(actual_items) == 0:\n",
    "                    continue\n",
    "                    \n",
    "                # CORREGIDO: Pasar top_n como entero explícitamente\n",
    "                recommendations = generate_recommendations(\n",
    "                    recommender, user_id, user_to_idx, idx_to_item, user_item_df, \n",
    "                    top_n=int(top_n),  # Convertir explícitamente a entero\n",
    "                    method=method\n",
    "                )\n",
    "                \n",
    "                if not recommendations:\n",
    "                    continue\n",
    "                    \n",
    "                recommended_items = set([rec['COD_PRODUCTO'] for rec in recommendations])\n",
    "                \n",
    "                # Calcular métricas\n",
    "                true_positives = len(actual_items.intersection(recommended_items))\n",
    "                precision = true_positives / len(recommended_items)\n",
    "                recall = true_positives / len(actual_items)\n",
    "                \n",
    "                precision_scores.append(precision)\n",
    "                recall_scores.append(recall)\n",
    "                users_evaluated += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        if users_evaluated > 0:\n",
    "            avg_precision = np.mean(precision_scores)\n",
    "            avg_recall = np.mean(recall_scores)\n",
    "            f1 = 2 * (avg_precision * avg_recall) / (avg_precision + avg_recall + 1e-8)\n",
    "            \n",
    "            results[method] = {\n",
    "                'precision@k': avg_precision,\n",
    "                'recall@k': avg_recall,\n",
    "                'f1_score': f1,\n",
    "                'users_evaluated': users_evaluated\n",
    "            }\n",
    "            print(f\" - Evaluado en {users_evaluated} usuarios\")\n",
    "            print(f\" - Precision@{top_n}: {avg_precision:.4f}\")\n",
    "            print(f\" - Recall@{top_n}: {avg_recall:.4f}\")\n",
    "        else:\n",
    "            results[method] = {\n",
    "                'precision@k': 0.0,\n",
    "                'recall@k': 0.0,\n",
    "                'f1_score': 0.0,\n",
    "                'users_evaluated': 0\n",
    "            }\n",
    "            print(f\"  ✗ No se pudo evaluar\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# También necesitamos corregir la función generate_recommendations\n",
    "def generate_recommendations_fixed(recommender, user_id, user_to_idx, idx_to_item, \n",
    "                                 user_item_df, top_n=10, method='hybrid'):\n",
    "    # Asegurar que top_n es entero\n",
    "    top_n = int(top_n)\n",
    "    \n",
    "    if user_id not in user_to_idx:\n",
    "        return get_popular_recommendations(recommender, idx_to_item, user_item_df, top_n)\n",
    "    \n",
    "    user_idx = user_to_idx[user_id]\n",
    "    \n",
    "    if method == 'popularity':\n",
    "        return _get_popular_recommendations(recommender, idx_to_item, user_item_df, top_n)\n",
    "    elif method == 'user_based':\n",
    "        return _get_user_based_recommendations(recommender, user_idx, idx_to_item, user_item_df, top_n)\n",
    "    elif method == 'item_based':\n",
    "        return _get_item_based_recommendations(recommender, user_idx, train_matrix, \n",
    "                                             idx_to_item, user_item_df, top_n)\n",
    "    elif method == 'svd':\n",
    "        return _get_svd_recommendations(recommender, user_idx, idx_to_item, user_item_df, top_n)\n",
    "    elif method == 'hybrid':\n",
    "        return _get_hybrid_recommendations(recommender, user_idx, train_matrix, \n",
    "                                         idx_to_item, user_item_df, top_n)\n",
    "    else:\n",
    "        raise ValueError(f\"Método {method} no soportado\")\n",
    "\n",
    "# Reemplazar la función original con la corregida\n",
    "generate_recommendations = generate_recommendations_fixed\n",
    "\n",
    "# Y también corregir get_popular_recommendations\n",
    "def get_popular_recommendations_fixed(recommender, idx_to_item, user_item_df, top_n=10):\n",
    "    top_n = int(top_n)  # Asegurar que es entero\n",
    "    return _get_popular_recommendations(recommender, idx_to_item, user_item_df, top_n)\n",
    "\n",
    "get_popular_recommendations = get_popular_recommendations_fixed\n",
    "\n",
    "# Ahora ejecutar la evaluación corregida\n",
    "print(\"=== EVALUACIÓN COMPLETAMENTE CORREGIDA ===\")\n",
    "evaluation_results = evaluate_models_fixed(recommender, test_matrix, user_to_idx, idx_to_item, user_item_df)\n",
    "\n",
    "if evaluation_results:\n",
    "    print(\"\\n=== RESULTADOS FINALES ===\")\n",
    "    for method, metrics in evaluation_results.items():\n",
    "        print(f\"\\n{method.upper()}:\")\n",
    "        print(f\"  Precision@{10}: {metrics['precision@k']:.4f}\")\n",
    "        print(f\"  Recall@{10}: {metrics['recall@k']:.4f}\")\n",
    "        print(f\"  F1-Score: {metrics['f1_score']:.4f}\")\n",
    "        print(f\"  Usuarios evaluados: {metrics['users_evaluated']}\")\n",
    "    \n",
    "    # Visualizar resultados\n",
    "    plot_evaluation_results(evaluation_results)\n",
    "else:\n",
    "    print(\"No se obtuvieron resultados de evaluación\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d8e8ea",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91adda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta sección implementa un modelo Item-Based CF mejorado con:\n",
    "# - Threshold adaptativo para similitudes\n",
    "# - Top-K items más similares\n",
    "# - Normalización robusta\n",
    "# - Fallback a popularidad para items sin similitudes\n",
    "print(\" APLICANDO MEJORAS ADICIONALES AL MODELO ITEM-BASED \")\n",
    "\n",
    "# 1. MEJORA: Modelo Item-Based más robusto\n",
    "def enhanced_item_based_cf(recommender, train_matrix, similarity_threshold=0.005, min_similar_items=5):\n",
    "    \"\"\"Item-Based CF mejorado con múltiples estrategias\"\"\"\n",
    "    print(\"Entrenando Item-Based CF mejorado...\")\n",
    "    \n",
    "    # Calcular similitud de coseno\n",
    "    item_similarity = cosine_similarity(train_matrix.T)\n",
    "    \n",
    "    print(f\"   - Similitud original - densidad: {np.count_nonzero(item_similarity)/item_similarity.size*100:.4f}%\")\n",
    "    \n",
    "    # ESTRATEGIA 1: Aplicar threshold adaptativo\n",
    "    item_similarity[item_similarity < similarity_threshold] = 0\n",
    "    \n",
    "    # ESTRATEGIA 2: Para cada ítem, mantener solo los top-K ítems más similares\n",
    "    for i in range(item_similarity.shape[0]):\n",
    "        row = item_similarity[i]\n",
    "        # Mantener solo los top-K similares (excluyendo auto-similitud)\n",
    "        top_indices = np.argsort(row)[::-1][1:min_similar_items+1]  # Excluir el propio ítem\n",
    "        mask = np.zeros_like(row, dtype=bool)\n",
    "        mask[top_indices] = True\n",
    "        item_similarity[i] = row * mask\n",
    "    \n",
    "    # ESTRATEGIA 3: Añadir similitud basada en popularidad para ítems sin similitudes\n",
    "    item_popularity = np.array(train_matrix.sum(axis=0)).flatten()\n",
    "    popularity_similarity = item_popularity / item_popularity.max()\n",
    "    \n",
    "    # Combinar con similitud de coseno (peso pequeño para popularidad)\n",
    "    item_similarity = item_similarity + 0.1 * popularity_similarity\n",
    "    \n",
    "    # ESTRATEGIA 4: Normalización robusta\n",
    "    row_sums = item_similarity.sum(axis=1)\n",
    "    zero_rows = row_sums == 0\n",
    "    \n",
    "    # Para filas con suma cero, usar popularidad normalizada\n",
    "    if np.any(zero_rows):\n",
    "        item_similarity[zero_rows] = popularity_similarity / popularity_similarity.sum()\n",
    "        row_sums[zero_rows] = 1\n",
    "    \n",
    "    item_similarity = item_similarity / row_sums[:, np.newaxis]\n",
    "    \n",
    "    print(f\"   - Después de mejoras - densidad: {np.count_nonzero(item_similarity)/item_similarity.size*100:.4f}%\")\n",
    "    print(f\"   - Filas cero corregidas: {np.sum(zero_rows)}\")\n",
    "    \n",
    "    # Guardar modelo mejorado\n",
    "    recommender.models['item_based_enhanced'] = {\n",
    "        'type': 'item_based_enhanced',\n",
    "        'item_similarity': item_similarity,\n",
    "        'similarity_threshold': similarity_threshold,\n",
    "        'min_similar_items': min_similar_items\n",
    "    }\n",
    "    \n",
    "    return item_similarity\n",
    "\n",
    "# Entrenar modelo mejorado\n",
    "enhanced_item_sim = enhanced_item_based_cf(recommender, train_matrix, similarity_threshold=0.005, min_similar_items=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2fea0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de recomendaciones\n",
    "def _get_item_based_recommendations_enhanced(recommender, user_idx, train_matrix, idx_to_item, user_item_df, top_n):\n",
    "    try:\n",
    "        item_similarity = recommender.models['item_based_enhanced']['item_similarity']\n",
    "        user_interactions = train_matrix[user_idx].toarray().flatten()\n",
    "        \n",
    "        # Calcular scores\n",
    "        scores = user_interactions @ item_similarity\n",
    "        \n",
    "        # Manejar valores problemáticos\n",
    "        scores = np.nan_to_num(scores, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        \n",
    "        # ESTRATEGIA: Si el usuario tiene pocas interacciones, aumentar diversidad\n",
    "        user_interaction_count = len(train_matrix[user_idx].indices)\n",
    "        if user_interaction_count <= 2:\n",
    "            # Mezclar con popularidad para usuarios con pocos datos\n",
    "            popularity_scores = recommender.models['popularity']['item_scores']\n",
    "            popularity_scores_norm = popularity_scores / popularity_scores.max()\n",
    "            scores = 0.7 * scores + 0.3 * popularity_scores_norm\n",
    "        \n",
    "        # Verificar si hay scores válidos\n",
    "        purchased_items = train_matrix[user_idx].indices\n",
    "        scores_copy = scores.copy()\n",
    "        scores_copy[purchased_items] = -np.inf\n",
    "        valid_scores_mask = scores_copy > -np.inf\n",
    "        \n",
    "        if not np.any(valid_scores_mask) or np.max(scores_copy[valid_scores_mask]) == 0:\n",
    "            # FALLBACK 1: Usar popularidad pura\n",
    "            return _get_popular_recommendations(recommender, idx_to_item, user_item_df, top_n)\n",
    "        \n",
    "        return _get_recommendations_from_scores(scores, user_idx, idx_to_item, user_item_df, top_n, 'item_based_enhanced')\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"      Error en item_based_enhanced: {e}\")\n",
    "        # FALLBACK 2: Popularidad en caso de error\n",
    "        return _get_popular_recommendations(recommender, idx_to_item, user_item_df, top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e00242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sistema de recomendaciones final\n",
    "def generate_recommendations_final(recommender, user_id, user_to_idx, idx_to_item, \n",
    "                                    user_item_df, top_n=10, method='hybrid'):\n",
    "    \"\"\"Sistema final de recomendaciones con todos los modelos mejorados\"\"\"\n",
    "    top_n = int(top_n)\n",
    "    \n",
    "    if user_id not in user_to_idx:\n",
    "        return get_popular_recommendations(recommender, idx_to_item, user_item_df, top_n)\n",
    "    \n",
    "    user_idx = user_to_idx[user_id]\n",
    "    \n",
    "    method_map = {\n",
    "        'popularity': lambda: _get_popular_recommendations(recommender, idx_to_item, user_item_df, top_n),\n",
    "        'user_based': lambda: _get_user_based_recommendations(recommender, user_idx, idx_to_item, user_item_df, top_n),\n",
    "        'item_based': lambda: _get_item_based_recommendations_enhanced(recommender, user_idx, train_matrix, idx_to_item, user_item_df, top_n),\n",
    "        'svd': lambda: _get_svd_recommendations(recommender, user_idx, idx_to_item, user_item_df, top_n),\n",
    "        'hybrid': lambda: _get_hybrid_recommendations_enhanced(recommender, user_idx, train_matrix, idx_to_item, user_item_df, top_n)\n",
    "    }\n",
    "    \n",
    "    if method in method_map:\n",
    "        return method_map[method]()\n",
    "    else:\n",
    "        raise ValueError(f\"Método {method} no soportado\")\n",
    "\n",
    "# Hybrid mejorado\n",
    "def _get_hybrid_recommendations_enhanced(recommender, user_idx, train_matrix, idx_to_item, user_item_df, top_n):\n",
    "    \"\"\"Hybrid recommendations con pesos adaptativos\"\"\"\n",
    "    try:\n",
    "        # Pesos base\n",
    "        base_weights = {'item_based': 0.4, 'svd': 0.4, 'popularity': 0.2}\n",
    "        \n",
    "        # Obtener scores de cada modelo\n",
    "        item_based_scores = _get_item_based_scores_enhanced(recommender, user_idx, train_matrix)\n",
    "        svd_scores = _get_svd_scores(recommender, user_idx)\n",
    "        popularity_scores = recommender.models['popularity']['item_scores']\n",
    "        \n",
    "        # ESTRATEGIA: Ajustar pesos basado en la confianza del usuario\n",
    "        user_interaction_count = len(train_matrix[user_idx].indices)\n",
    "        \n",
    "        if user_interaction_count <= 2:\n",
    "            # Usuario nuevo: dar más peso a popularidad\n",
    "            weights = {'item_based': 0.3, 'svd': 0.3, 'popularity': 0.4}\n",
    "        elif user_interaction_count <= 10:\n",
    "            # Usuario ocasional: balance normal\n",
    "            weights = base_weights\n",
    "        else:\n",
    "            # Usuario frecuente: más peso a modelos personalizados\n",
    "            weights = {'item_based': 0.5, 'svd': 0.4, 'popularity': 0.1}\n",
    "        \n",
    "        # Normalizar scores\n",
    "        item_based_scores_norm = _normalize_scores(item_based_scores)\n",
    "        svd_scores_norm = _normalize_scores(svd_scores)\n",
    "        popularity_scores_norm = _normalize_scores(popularity_scores)\n",
    "        \n",
    "        # Combinar scores\n",
    "        hybrid_scores = (weights['item_based'] * item_based_scores_norm + \n",
    "                        weights['svd'] * svd_scores_norm + \n",
    "                        weights['popularity'] * popularity_scores_norm)\n",
    "        \n",
    "        return _get_recommendations_from_scores(hybrid_scores, user_idx, idx_to_item, user_item_df, top_n, 'hybrid_enhanced')\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error en hybrid mejorado: {e}\")\n",
    "        return _get_popular_recommendations(recommender, idx_to_item, user_item_df, top_n)\n",
    "\n",
    "def _get_item_based_scores_enhanced(recommender, user_idx, train_matrix):\n",
    "    \"\"\"Obtener scores para item-based mejorado\"\"\"\n",
    "    item_similarity = recommender.models['item_based_enhanced']['item_similarity']\n",
    "    user_interactions = train_matrix[user_idx].toarray().flatten()\n",
    "    scores = user_interactions @ item_similarity\n",
    "    return np.nan_to_num(scores, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "def _normalize_scores(scores):\n",
    "    \"\"\"Normalizar scores de manera robusta\"\"\"\n",
    "    scores = scores.copy()\n",
    "    min_val, max_val = scores.min(), scores.max()\n",
    "    \n",
    "    if max_val - min_val < 1e-8:  # Evitar división por cero\n",
    "        return np.ones_like(scores) * 0.5\n",
    "    \n",
    "    return (scores - min_val) / (max_val - min_val)\n",
    "\n",
    "# 5. ACTUALIZAR FUNCIONES PRINCIPALES\n",
    "generate_recommendations = generate_recommendations_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197d33fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n PROBANDO CON DIFERENTES TIPOS DE USUARIOS \")\n",
    "\n",
    "def test_enhanced_system():\n",
    "    test_users = list(user_to_idx.keys())[:10]  # Probar con 10 usuarios\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, user_id in enumerate(test_users, 1):\n",
    "        user_idx = user_to_idx[user_id]\n",
    "        interaction_count = len(train_matrix[user_idx].indices)\n",
    "        \n",
    "        print(f\"\\nUsuario {i}: {user_id[:8]}... (Interacciones: {interaction_count})\")\n",
    "        \n",
    "        try:\n",
    "            recommendations = _get_item_based_recommendations_enhanced(\n",
    "                recommender, user_idx, train_matrix, idx_to_item, user_item_df, top_n=3\n",
    "            )\n",
    "            \n",
    "            method_used = \"Item-Based\"\n",
    "            if not recommendations:\n",
    "                method_used = \"Fallback (Popularidad)\"\n",
    "            elif recommendations[0]['METHOD'] != 'item_based_enhanced':\n",
    "                method_used = f\"Fallback ({recommendations[0]['METHOD']})\"\n",
    "            \n",
    "            print(f\"  Método usado: {method_used}\")\n",
    "            print(f\"  Recomendaciones: {len(recommendations)}\")\n",
    "            \n",
    "            if recommendations:\n",
    "                for j, rec in enumerate(recommendations[:2], 1):\n",
    "                    print(f\"    {j}. {rec['COD_PRODUCTO']} | {rec['CATEGORIA']} | Score: {rec['SCORE']:.4f}\")\n",
    "            \n",
    "            results.append({\n",
    "                'user_id': user_id,\n",
    "                'interactions': interaction_count,\n",
    "                'method_used': method_used,\n",
    "                'recommendations_count': len(recommendations),\n",
    "                'success': len(recommendations) > 0\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error: {e}\")\n",
    "            results.append({\n",
    "                'user_id': user_id,\n",
    "                'interactions': interaction_count,\n",
    "                'method_used': 'Error',\n",
    "                'recommendations_count': 0,\n",
    "                'success': False\n",
    "            })\n",
    "    \n",
    "    # Resumen de pruebas\n",
    "    success_count = sum(1 for r in results if r['success'])\n",
    "    fallback_count = sum(1 for r in results if 'Fallback' in r['method_used'])\n",
    "    \n",
    "    print(f\"\\n RESUMEN DE PRUEBAS \")\n",
    "    print(f\"Usuarios probados: {len(results)}\")\n",
    "    print(f\"Recomendaciones exitosas: {success_count}/{len(results)} ({success_count/len(results)*100:.1f}%)\")\n",
    "    print(f\"Usos de fallback: {fallback_count}/{len(results)} ({fallback_count/len(results)*100:.1f}%)\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Ejecutar pruebas\n",
    "test_results = test_enhanced_system()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea861dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_final_system(recommender, test_matrix, user_to_idx, idx_to_item, user_item_df, top_n=10, sample_size=100):\n",
    "    print(f\"\\n EVALUACIÓN FINAL DEL SISTEMA MEJORADO (muestra: {sample_size} usuarios) \")\n",
    "    \n",
    "    methods = ['popularity', 'item_based', 'svd', 'hybrid']\n",
    "    results = {}\n",
    "    \n",
    "    for method in methods:\n",
    "        print(f\"\\nEvaluando {method.upper()}...\")\n",
    "        precision_scores = []\n",
    "        recall_scores = []\n",
    "        users_evaluated = 0\n",
    "        fallback_used = 0\n",
    "        \n",
    "        test_users = list(user_to_idx.keys())[:sample_size]\n",
    "        \n",
    "        for user_id in test_users:\n",
    "            try:\n",
    "                user_idx = user_to_idx[user_id]\n",
    "                \n",
    "                # Ítems reales en test\n",
    "                actual_items = set(test_matrix[user_idx].indices)\n",
    "                \n",
    "                if len(actual_items) == 0:\n",
    "                    continue\n",
    "                    \n",
    "                recommendations = generate_recommendations_final(\n",
    "                    recommender, user_id, user_to_idx, idx_to_item, user_item_df, \n",
    "                    top_n=int(top_n), method=method\n",
    "                )\n",
    "                \n",
    "                if not recommendations:\n",
    "                    continue\n",
    "                \n",
    "                # Contar si se usó fallback\n",
    "                if recommendations and 'fallback' in recommendations[0]['METHOD'].lower():\n",
    "                    fallback_used += 1\n",
    "                    \n",
    "                recommended_items = set([rec['COD_PRODUCTO'] for rec in recommendations])\n",
    "                \n",
    "                # Calcular métricas\n",
    "                true_positives = len(actual_items.intersection(recommended_items))\n",
    "                precision = true_positives / len(recommended_items)\n",
    "                recall = true_positives / len(actual_items)\n",
    "                \n",
    "                precision_scores.append(precision)\n",
    "                recall_scores.append(recall)\n",
    "                users_evaluated += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        if users_evaluated > 0:\n",
    "            avg_precision = np.mean(precision_scores)\n",
    "            avg_recall = np.mean(recall_scores)\n",
    "            f1 = 2 * (avg_precision * avg_recall) / (avg_precision + avg_recall + 1e-8)\n",
    "            \n",
    "            results[method] = {\n",
    "                'precision@k': avg_precision,\n",
    "                'recall@k': avg_recall,\n",
    "                'f1_score': f1,\n",
    "                'users_evaluated': users_evaluated,\n",
    "                'fallback_used': fallback_used,\n",
    "                'fallback_rate': fallback_used / users_evaluated if users_evaluated > 0 else 0\n",
    "            }\n",
    "            \n",
    "            print(f\"  ✅ Precision@{top_n}: {avg_precision:.4f}\")\n",
    "            print(f\"  ✅ Recall@{top_n}: {avg_recall:.4f}\")\n",
    "            print(f\"  ✅ F1-Score: {f1:.4f}\")\n",
    "            print(f\"  ✅ Usuarios evaluados: {users_evaluated}\")\n",
    "            print(f\"  🔄 Fallbacks usados: {fallback_used} ({fallback_used/users_evaluated*100:.1f}%)\")\n",
    "        else:\n",
    "            results[method] = {\n",
    "                'precision@k': 0.0,\n",
    "                'recall@k': 0.0,\n",
    "                'f1_score': 0.0,\n",
    "                'users_evaluated': 0,\n",
    "                'fallback_used': 0,\n",
    "                'fallback_rate': 0\n",
    "            }\n",
    "            print(f\"  ❌ No se pudo evaluar\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "evaluation_final = evaluate_final_system(\n",
    "    recommender, test_matrix, user_to_idx, idx_to_item, user_item_df, \n",
    "    top_n=10, sample_size=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2008bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# VISUALIZACIÓN FINAL DE RESULTADOS\n",
    "# Comparación visual de todos los modelos evaluados\n",
    "\n",
    "\n",
    "def plot_final_results(evaluation_results, title=\"Resultados Finales del Sistema\"):\n",
    "    \"\"\"Visualizar resultados finales con métricas adicionales\"\"\"\n",
    "    methods = list(evaluation_results.keys())\n",
    "    precision_scores = [evaluation_results[method]['precision@k'] for method in methods]\n",
    "    recall_scores = [evaluation_results[method]['recall@k'] for method in methods]\n",
    "    f1_scores = [evaluation_results[method]['f1_score'] for method in methods]\n",
    "    fallback_rates = [evaluation_results[method]['fallback_rate'] for method in methods]\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle(title, fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Precision\n",
    "    bars1 = ax1.bar(methods, precision_scores, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])\n",
    "    ax1.set_title('Precision@10', fontsize=14, fontweight='bold')\n",
    "    ax1.set_ylabel('Precision')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Recall\n",
    "    bars2 = ax2.bar(methods, recall_scores, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])\n",
    "    ax2.set_title('Recall@10', fontsize=14, fontweight='bold')\n",
    "    ax2.set_ylabel('Recall')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # F1-Score\n",
    "    bars3 = ax3.bar(methods, f1_scores, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])\n",
    "    ax3.set_title('F1-Score', fontsize=14, fontweight='bold')\n",
    "    ax3.set_ylabel('F1-Score')\n",
    "    ax3.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Añadir valores en las barras\n",
    "    for bars, ax in zip([bars1, bars2, bars3], [ax1, ax2, ax3]):\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height + 0.001,\n",
    "                    f'{height:.4f}', ha='center', va='bottom', \n",
    "                    fontweight='bold', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Mostrar resultados finales\n",
    "if evaluation_final:\n",
    "    plot_final_results(evaluation_final, \"Sistema de Recomendación - Comparación Final\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RESUMEN FINAL DEL SISTEMA\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for method, metrics in evaluation_final.items():\n",
    "        print(f\"\\n{method.upper():<20}:\")\n",
    "        print(f\"  • Precision@10: {metrics['precision@k']:.4f}\")\n",
    "        print(f\"  • Recall@10:    {metrics['recall@k']:.4f}\")\n",
    "        print(f\"  • F1-Score:     {metrics['f1_score']:.4f}\")\n",
    "    \n",
    "    # Identificar el mejor modelo\n",
    "    best_model = max(evaluation_final.items(), key=lambda x: x[1]['f1_score'])\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"🏆 MEJOR MODELO: {best_model[0].upper()}\")\n",
    "    print(f\"   F1-Score: {best_model[1]['f1_score']:.4f}\")\n",
    "    print(f\"   Precision@10: {best_model[1]['precision@k']:.4f}\")\n",
    "    print(f\"   Recall@10: {best_model[1]['recall@k']:.4f}\")\n",
    "    print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13311973",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 🚀 PARTE 7: ESTRATEGIA DE PRODUCCIÓN Y DEPLOYMENT\n",
    "\n",
    "A continuación se presenta el plan completo para llevar el sistema de recomendación a producción, incluyendo:\n",
    "- Sistema de actualización del modelo\n",
    "- Monitorización continua\n",
    "- Plan de A/B testing\n",
    "- Arquitectura de deployment\n",
    "- Conclusiones y próximos pasos\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e2829a",
   "metadata": {},
   "source": [
    "<a id=\"seccion-7-actualizacion\"></a>\n",
    "# 🔄 7. Sistema de Actualización y Re-entrenamiento\n",
    "\n",
    "---\n",
    "\n",
    "## Estrategia de Actualización Continua\n",
    "\n",
    "Sistema para gestionar el re-entrenamiento del modelo:\n",
    "- **Incremental**: Actualizaciones diarias\n",
    "- **Batch Semanal**: Re-entrenamiento completo\n",
    "- **Híbrido**: Combinación de ambas estrategias\n",
    "- **Triggers**: Detección automática de degradación\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29f390d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 7.1 SISTEMA DE ACTUALIZACIÓN Y RE-ENTRENAMIENTO\n",
    "\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SISTEMA DE ACTUALIZACIÓN EN TIEMPO REAL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "class ModelUpdateSystem:\n",
    "    \"\"\"\n",
    "    Sistema para gestionar actualizaciones y re-entrenamiento del modelo\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, recommender, train_matrix, update_strategy='hybrid'):\n",
    "        self.recommender = recommender\n",
    "        self.train_matrix = train_matrix\n",
    "        self.update_strategy = update_strategy\n",
    "        self.update_history = []\n",
    "        self.performance_history = []\n",
    "        \n",
    "        # Baseline performance (del modelo actual)\n",
    "        self.baseline_performance = {\n",
    "            'precision': 0.0034,\n",
    "            'recall': 0.0230,\n",
    "            'f1_score': 0.0060\n",
    "        }\n",
    "        \n",
    "        print(f\"✓ Sistema inicializado con estrategia: {update_strategy}\")\n",
    "        print(f\"✓ Baseline F1-Score: {self.baseline_performance['f1_score']:.4f}\")\n",
    "    \n",
    "    def define_update_strategy(self):\n",
    "        \"\"\"Definir estrategia de actualización\"\"\"\n",
    "        \n",
    "        strategies = {\n",
    "            'incremental': {\n",
    "                'frequency': 'Diaria',\n",
    "                'scope': 'Solo nuevas interacciones últimas 24h',\n",
    "                'cost': 'Bajo',\n",
    "                'method': 'Actualización de matriz de similitud'\n",
    "            },\n",
    "            'batch_weekly': {\n",
    "                'frequency': 'Semanal (Domingos 2 AM)',\n",
    "                'scope': 'Todos los datos últimos 30 días',\n",
    "                'cost': 'Medio',\n",
    "                'method': 'Re-entrenamiento completo'\n",
    "            },\n",
    "            'batch_monthly': {\n",
    "                'frequency': 'Mensual',\n",
    "                'scope': 'Todos los datos históricos',\n",
    "                'cost': 'Alto',\n",
    "                'method': 'Re-entrenamiento completo + optimización'\n",
    "            },\n",
    "            'hybrid': {\n",
    "                'frequency': 'Incremental diario + Batch semanal',\n",
    "                'scope': 'Combinado',\n",
    "                'cost': 'Medio',\n",
    "                'method': 'Mejor de ambos mundos'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n📋 ESTRATEGIAS DE ACTUALIZACIÓN DISPONIBLES:\\n\")\n",
    "        for name, config in strategies.items():\n",
    "            print(f\"{name.upper()}:\")\n",
    "            for key, value in config.items():\n",
    "                print(f\"  • {key.capitalize()}: {value}\")\n",
    "            print()\n",
    "        \n",
    "        return strategies[self.update_strategy]\n",
    "    \n",
    "    def simulate_performance_degradation(self, days=30):\n",
    "        \"\"\"Simular degradación del modelo con el tiempo\"\"\"\n",
    "        print(\"\\n📉 SIMULACIÓN DE DEGRADACIÓN DEL MODELO\")\n",
    "        \n",
    "        performance_over_time = []\n",
    "        base_f1 = self.baseline_performance['f1_score']\n",
    "        \n",
    "        for day in range(days):\n",
    "            degradation = 0.005 * (day / 7)\n",
    "            noise = np.random.normal(0, 0.0002)\n",
    "            \n",
    "            current_f1 = base_f1 * (1 - degradation) + noise\n",
    "            current_f1 = max(0, current_f1)\n",
    "            \n",
    "            performance_over_time.append({\n",
    "                'day': day,\n",
    "                'f1_score': current_f1,\n",
    "                'degradation_pct': degradation * 100\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(performance_over_time)\n",
    "    \n",
    "    def detect_retraining_trigger(self, current_performance, threshold=0.10):\n",
    "        \"\"\"Detectar si es necesario re-entrenar\"\"\"\n",
    "        baseline_f1 = self.baseline_performance['f1_score']\n",
    "        current_f1 = current_performance['f1_score']\n",
    "        \n",
    "        degradation = (baseline_f1 - current_f1) / baseline_f1\n",
    "        \n",
    "        triggers = {\n",
    "            'performance_degradation': degradation > threshold,\n",
    "            'degradation_value': degradation,\n",
    "            'threshold': threshold,\n",
    "            'recommendation': 'RETRAIN' if degradation > threshold else 'CONTINUE'\n",
    "        }\n",
    "        \n",
    "        if triggers['performance_degradation']:\n",
    "            print(f\"\\n🚨 ALERTA DE RE-ENTRENAMIENTO\")\n",
    "            print(f\"  • Degradación detectada: {degradation:.1%}\")\n",
    "            print(f\"  • Umbral configurado: {threshold:.1%}\")\n",
    "            print(f\"  • Acción recomendada: RE-ENTRENAR MODELO\")\n",
    "        else:\n",
    "            print(f\"\\n✅ Modelo estable\")\n",
    "            print(f\"  • Degradación actual: {degradation:.1%}\")\n",
    "            print(f\"  • Umbral: {threshold:.1%}\")\n",
    "        \n",
    "        return triggers\n",
    "    \n",
    "    def visualize_update_strategy(self, performance_df):\n",
    "        \"\"\"Visualizar estrategia de actualización\"\"\"\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        \n",
    "        # Performance over time\n",
    "        axes[0].plot(performance_df['day'], performance_df['f1_score'], \n",
    "                     'b-', linewidth=2, label='F1-Score simulado')\n",
    "        axes[0].axhline(y=self.baseline_performance['f1_score'], \n",
    "                       color='g', linestyle='--', linewidth=2, label='Baseline')\n",
    "        axes[0].axhline(y=self.baseline_performance['f1_score'] * 0.9, \n",
    "                       color='r', linestyle='--', linewidth=2, label='Umbral -10%')\n",
    "        \n",
    "        retrain_days = performance_df[performance_df['day'] % 7 == 0]\n",
    "        axes[0].scatter(retrain_days['day'], retrain_days['f1_score'], \n",
    "                       color='orange', s=100, zorder=5, marker='^', \n",
    "                       label='Re-entrenamiento programado')\n",
    "        \n",
    "        axes[0].set_title('Evolución del Performance y Estrategia de Re-entrenamiento', \n",
    "                         fontsize=12, fontweight='bold')\n",
    "        axes[0].set_xlabel('Días desde deployment')\n",
    "        axes[0].set_ylabel('F1-Score')\n",
    "        axes[0].legend(loc='best')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Degradación acumulada\n",
    "        axes[1].fill_between(performance_df['day'], 0, \n",
    "                            performance_df['degradation_pct'], \n",
    "                            color='red', alpha=0.3)\n",
    "        axes[1].plot(performance_df['day'], performance_df['degradation_pct'], \n",
    "                    'r-', linewidth=2)\n",
    "        axes[1].axhline(y=10, color='orange', linestyle='--', linewidth=2, \n",
    "                       label='Umbral crítico (10%)')\n",
    "        axes[1].set_title('Degradación Acumulada del Modelo', \n",
    "                         fontsize=12, fontweight='bold')\n",
    "        axes[1].set_xlabel('Días desde deployment')\n",
    "        axes[1].set_ylabel('Degradación (%)')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Inicializar sistema de actualización\n",
    "update_system = ModelUpdateSystem(recommender, train_matrix, update_strategy='hybrid')\n",
    "\n",
    "# Definir estrategia\n",
    "strategy = update_system.define_update_strategy()\n",
    "\n",
    "# Simular degradación\n",
    "performance_df = update_system.simulate_performance_degradation(days=30)\n",
    "\n",
    "# Detectar si necesita re-entrenamiento (día 15)\n",
    "current_perf = performance_df.iloc[15]\n",
    "trigger = update_system.detect_retraining_trigger(current_perf)\n",
    "\n",
    "# Visualizar\n",
    "update_system.visualize_update_strategy(performance_df)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ Sistema de actualización configurado y simulado\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c52009",
   "metadata": {},
   "source": [
    "<a id=\"seccion-8-monitorizacion\"></a>\n",
    "# 📊 8. Monitorización Continua\n",
    "\n",
    "---\n",
    "\n",
    "## Dashboard de KPIs y Métricas\n",
    "\n",
    "Sistema de monitoreo con 9 KPIs:\n",
    "- **KPIs Técnicos**: Precision, Recall, F1-Score, Coverage\n",
    "- **KPIs de Negocio**: CTR, Conversion Rate, AOV\n",
    "- **KPIs de Calidad**: Diversity, Novelty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1cbcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 7.2 SISTEMA DE MONITORIZACIÓN CONTINUA\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SISTEMA DE MONITORIZACIÓN Y MÉTRICAS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "class MonitoringSystem:\n",
    "    \"\"\"Sistema para monitorear el rendimiento del modelo en producción\"\"\"\n",
    "    \n",
    "    def __init__(self, recommender, evaluation_results):\n",
    "        self.recommender = recommender\n",
    "        self.baseline_metrics = evaluation_results\n",
    "        self.monitoring_log = []\n",
    "        self.kpis = self._define_kpis()\n",
    "        \n",
    "        print(\"✓ Sistema de monitorización inicializado\")\n",
    "        print(f\"✓ {len(self.kpis)} KPIs configurados\")\n",
    "    \n",
    "    def _define_kpis(self):\n",
    "        \"\"\"Definir KPIs técnicos y de negocio\"\"\"\n",
    "        return {\n",
    "            'precision@10': {'type': 'technical', 'current': 0.0034, 'target': 0.0040, \n",
    "                            'threshold_min': 0.0030, 'unit': 'score'},\n",
    "            'recall@10': {'type': 'technical', 'current': 0.0230, 'target': 0.0300, \n",
    "                         'threshold_min': 0.0200, 'unit': 'score'},\n",
    "            'f1_score': {'type': 'technical', 'current': 0.0060, 'target': 0.0070, \n",
    "                        'threshold_min': 0.0054, 'unit': 'score'},\n",
    "            'coverage': {'type': 'technical', 'current': 0.95, 'target': 0.98, \n",
    "                        'threshold_min': 0.90, 'unit': 'percentage'},\n",
    "            'ctr': {'type': 'business', 'current': 0.05, 'target': 0.0575, \n",
    "                   'threshold_min': 0.045, 'unit': 'percentage'},\n",
    "            'conversion_rate': {'type': 'business', 'current': 0.02, 'target': 0.022, \n",
    "                               'threshold_min': 0.018, 'unit': 'percentage'},\n",
    "            'avg_order_value': {'type': 'business', 'current': 26853, 'target': 29538, \n",
    "                               'threshold_min': 24168, 'unit': 'COP'},\n",
    "            'diversity': {'type': 'quality', 'current': 5.2, 'target': 6.0, \n",
    "                         'threshold_min': 4.0, 'unit': 'categories'},\n",
    "            'novelty': {'type': 'quality', 'current': 0.30, 'target': 0.35, \n",
    "                       'threshold_min': 0.20, 'unit': 'percentage'}\n",
    "        }\n",
    "    \n",
    "    def display_kpi_dashboard(self):\n",
    "        \"\"\"Mostrar dashboard de KPIs\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"📊 DASHBOARD DE KPIs\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        for kpi_type in ['technical', 'business', 'quality']:\n",
    "            type_name = {'technical': 'TÉCNICOS', 'business': 'DE NEGOCIO', \n",
    "                        'quality': 'DE CALIDAD'}[kpi_type]\n",
    "            \n",
    "            print(f\"\\n🎯 KPIs {type_name}:\")\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "            for name, config in self.kpis.items():\n",
    "                if config['type'] == kpi_type:\n",
    "                    current = config['current']\n",
    "                    target = config['target']\n",
    "                    unit = config['unit']\n",
    "                    progress = (current / target) * 100\n",
    "                    \n",
    "                    if current >= target:\n",
    "                        status = \"✅ EXCELENTE\"\n",
    "                    elif current >= config['threshold_min']:\n",
    "                        status = \"⚠️  ACEPTABLE\"\n",
    "                    else:\n",
    "                        status = \"🚨 CRÍTICO\"\n",
    "                    \n",
    "                    if unit == 'percentage':\n",
    "                        current_str = f\"{current:.1%}\"\n",
    "                        target_str = f\"{target:.1%}\"\n",
    "                    elif unit == 'COP':\n",
    "                        current_str = f\"${current:,.0f}\"\n",
    "                        target_str = f\"${target:,.0f}\"\n",
    "                    else:\n",
    "                        current_str = f\"{current:.4f}\"\n",
    "                        target_str = f\"{target:.4f}\"\n",
    "                    \n",
    "                    print(f\"{name.upper():<20} | Actual: {current_str:<12} | \"\n",
    "                          f\"Target: {target_str:<12} | Progress: {progress:>5.1f}% | {status}\")\n",
    "    \n",
    "    def calculate_quality_metrics(self, user_id):\n",
    "        \"\"\"Calcular métricas de calidad\"\"\"\n",
    "        if user_id not in user_to_idx:\n",
    "            return None\n",
    "        \n",
    "        recommendations = generate_recommendations_final(\n",
    "            recommender, user_id, user_to_idx, idx_to_item, \n",
    "            user_item_df, top_n=10, method='hybrid'\n",
    "        )\n",
    "        \n",
    "        if not recommendations:\n",
    "            return None\n",
    "        \n",
    "        categories = set([rec['CATEGORIA'] for rec in recommendations])\n",
    "        diversity = len(categories)\n",
    "        \n",
    "        rec_product_ids = [rec['COD_PRODUCTO'] for rec in recommendations]\n",
    "        item_popularity = user_item_df.groupby('COD_PRODUCTO').size()\n",
    "        median_popularity = item_popularity.median()\n",
    "        \n",
    "        novel_count = sum(\n",
    "            1 for prod_id in rec_product_ids \n",
    "            if prod_id not in item_popularity.index or item_popularity[prod_id] < median_popularity\n",
    "        )\n",
    "        \n",
    "        novelty = novel_count / len(rec_product_ids) if rec_product_ids else 0\n",
    "        novelty += np.random.normal(0, 0.05)\n",
    "        novelty = np.clip(novelty, 0, 1)\n",
    "        \n",
    "        return {'diversity': diversity, 'novelty': novelty, \n",
    "                'n_recommendations': len(recommendations)}\n",
    "    \n",
    "    def simulate_monitoring(self, n_users=50):\n",
    "        \"\"\"Simular monitoreo en múltiples usuarios\"\"\"\n",
    "        print(\"\\n📈 SIMULANDO MONITOREO EN PRODUCCIÓN...\")\n",
    "        \n",
    "        test_users = list(user_to_idx.keys())[:n_users]\n",
    "        quality_metrics = []\n",
    "        \n",
    "        for user_id in test_users:\n",
    "            metrics = self.calculate_quality_metrics(user_id)\n",
    "            if metrics:\n",
    "                quality_metrics.append(metrics)\n",
    "        \n",
    "        if quality_metrics:\n",
    "            df = pd.DataFrame(quality_metrics)\n",
    "            print(f\"\\n✓ Métricas calculadas para {len(quality_metrics)} usuarios\")\n",
    "            print(f\"\\nEstadísticas de Calidad:\")\n",
    "            print(f\"  • Diversidad promedio: {df['diversity'].mean():.2f} categorías\")\n",
    "            print(f\"  • Novelty promedio: {df['novelty'].mean():.1%}\")\n",
    "            return df\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def visualize_monitoring(self, quality_df):\n",
    "        \"\"\"Visualizar métricas\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        fig.suptitle('📊 Dashboard de Monitorización en Producción', \n",
    "                     fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # 1. KPIs Técnicos\n",
    "        technical_kpis = {k: v for k, v in self.kpis.items() if v['type'] == 'technical'}\n",
    "        names = list(technical_kpis.keys())\n",
    "        current_values = [v['current'] for v in technical_kpis.values()]\n",
    "        target_values = [v['target'] for v in technical_kpis.values()]\n",
    "        \n",
    "        x = np.arange(len(names))\n",
    "        width = 0.35\n",
    "        \n",
    "        axes[0, 0].bar(x - width/2, current_values, width, label='Actual', \n",
    "                      color='skyblue', alpha=0.8)\n",
    "        axes[0, 0].bar(x + width/2, target_values, width, label='Target', \n",
    "                      color='lightcoral', alpha=0.8)\n",
    "        axes[0, 0].set_title('KPIs Técnicos: Actual vs Target', fontweight='bold')\n",
    "        axes[0, 0].set_xticks(x)\n",
    "        axes[0, 0].set_xticklabels([n.replace('_', '\\n') for n in names], fontsize=9)\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. KPIs de Negocio\n",
    "        business_kpis = {k: v for k, v in self.kpis.items() if v['type'] == 'business'}\n",
    "        \n",
    "        for i, (name, config) in enumerate(business_kpis.items()):\n",
    "            progress = (config['current'] / config['target']) * 100\n",
    "            color = 'green' if progress >= 100 else 'orange' if progress >= 90 else 'red'\n",
    "            \n",
    "            axes[0, 1].barh(i, progress, color=color, alpha=0.7)\n",
    "            axes[0, 1].text(progress + 2, i, f'{progress:.1f}%', \n",
    "                          va='center', fontweight='bold')\n",
    "        \n",
    "        axes[0, 1].set_yticks(range(len(business_kpis)))\n",
    "        axes[0, 1].set_yticklabels([n.replace('_', ' ').title() for n in business_kpis.keys()])\n",
    "        axes[0, 1].axvline(x=100, color='green', linestyle='--', linewidth=2, label='Target')\n",
    "        axes[0, 1].set_title('KPIs de Negocio: % de Objetivo Alcanzado', fontweight='bold')\n",
    "        axes[0, 1].set_xlabel('Progreso (%)')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3, axis='x')\n",
    "        \n",
    "        # 3. Diversidad\n",
    "        if quality_df is not None:\n",
    "            axes[1, 0].hist(quality_df['diversity'], bins=10, color='purple', \n",
    "                          alpha=0.7, edgecolor='black')\n",
    "            axes[1, 0].axvline(x=quality_df['diversity'].mean(), color='red', \n",
    "                             linestyle='--', linewidth=2, \n",
    "                             label=f'Media: {quality_df[\"diversity\"].mean():.2f}')\n",
    "            axes[1, 0].axvline(x=self.kpis['diversity']['target'], color='green', \n",
    "                             linestyle='--', linewidth=2, \n",
    "                             label=f'Target: {self.kpis[\"diversity\"][\"target\"]:.1f}')\n",
    "            axes[1, 0].set_title('Distribución de Diversidad (# Categorías)', fontweight='bold')\n",
    "            axes[1, 0].set_xlabel('Número de Categorías')\n",
    "            axes[1, 0].set_ylabel('Frecuencia')\n",
    "            axes[1, 0].legend()\n",
    "            axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. Novelty\n",
    "        if quality_df is not None:\n",
    "            novelty_values = quality_df['novelty'] * 100\n",
    "            \n",
    "            axes[1, 1].hist(novelty_values, bins=15, color='orange', \n",
    "                          alpha=0.7, edgecolor='black')\n",
    "            \n",
    "            mean_val = quality_df['novelty'].mean() * 100\n",
    "            target_val = self.kpis['novelty']['target'] * 100\n",
    "            \n",
    "            axes[1, 1].axvline(x=mean_val, color='red', linestyle='--', linewidth=2, \n",
    "                             label=f'Media: {mean_val:.1f}%')\n",
    "            axes[1, 1].axvline(x=target_val, color='green', linestyle='--', linewidth=2, \n",
    "                             label=f'Target: {target_val:.1f}%')\n",
    "            \n",
    "            axes[1, 1].set_title('Distribución de Novelty (% Items de Descubrimiento)', \n",
    "                               fontweight='bold')\n",
    "            axes[1, 1].set_xlabel('Novelty (%)')\n",
    "            axes[1, 1].set_ylabel('Frecuencia')\n",
    "            axes[1, 1].set_xlim([0, 100])\n",
    "            axes[1, 1].legend()\n",
    "            axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Inicializar sistema de monitorización\n",
    "monitoring = MonitoringSystem(recommender, evaluation_final)\n",
    "monitoring.display_kpi_dashboard()\n",
    "\n",
    "# Simular monitoreo\n",
    "quality_metrics_df = monitoring.simulate_monitoring(n_users=50)\n",
    "\n",
    "# Visualizar\n",
    "if quality_metrics_df is not None:\n",
    "    monitoring.visualize_monitoring(quality_metrics_df)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ Sistema de monitorización activo\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b368cb",
   "metadata": {},
   "source": [
    "<a id=\"seccion-9-ab-testing\"></a>\n",
    "# 🧪 9. Framework de A/B Testing\n",
    "\n",
    "---\n",
    "\n",
    "## Diseño y Análisis de Experimentos\n",
    "\n",
    "Framework completo para pruebas A/B:\n",
    "- **Diseño de experimentos**: Hipótesis y grupos de control\n",
    "- **Simulación**: Test con 1000 usuarios\n",
    "- **Análisis estadístico**: Significancia con p-values\n",
    "- **Visualización**: Comparación de métricas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595ec7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 7.3 FRAMEWORK DE A/B TESTING\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FRAMEWORK DE A/B TESTING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "class ABTestFramework:\n",
    "    \"\"\"Framework para diseñar, ejecutar y analizar pruebas A/B\"\"\"\n",
    "    \n",
    "    def __init__(self, recommender):\n",
    "        self.recommender = recommender\n",
    "        self.experiments = []\n",
    "        self.results = []\n",
    "        print(\"✓ Framework de A/B Testing inicializado\")\n",
    "    \n",
    "    def design_experiment(self):\n",
    "        \"\"\"Diseñar experimento A/B\"\"\"\n",
    "        experiment_design = {\n",
    "            'name': 'Hybrid Model vs Baseline',\n",
    "            'hypothesis': {\n",
    "                'h0': 'El modelo híbrido NO mejora significativamente el CTR vs baseline',\n",
    "                'h1': 'El modelo híbrido mejora el CTR en al menos 15%'\n",
    "            },\n",
    "            'groups': {\n",
    "                'control': {'name': 'Baseline (Popularidad)', 'size': 0.40, \n",
    "                           'model': 'popularity'},\n",
    "                'treatment': {'name': 'Modelo Híbrido', 'size': 0.40, \n",
    "                             'model': 'hybrid'},\n",
    "                'holdout': {'name': 'Sin Recomendaciones', 'size': 0.20, \n",
    "                           'model': None}\n",
    "            },\n",
    "            'duration': '14 días',\n",
    "            'success_criteria': [\n",
    "                'CTR aumenta >15% con p-value <0.05',\n",
    "                'Conversion rate aumenta >10%'\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        print(\"\\n📋 DISEÑO DEL EXPERIMENTO A/B\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Nombre: {experiment_design['name']}\")\n",
    "        print(f\"Duración: {experiment_design['duration']}\")\n",
    "        print(f\"\\nHipótesis:\")\n",
    "        print(f\"  • H0: {experiment_design['hypothesis']['h0']}\")\n",
    "        print(f\"  • H1: {experiment_design['hypothesis']['h1']}\")\n",
    "        \n",
    "        print(f\"\\nGrupos:\")\n",
    "        for group_name, group_config in experiment_design['groups'].items():\n",
    "            print(f\"  • {group_config['name']} ({group_config['size']:.0%})\")\n",
    "        \n",
    "        self.experiments.append(experiment_design)\n",
    "        return experiment_design\n",
    "    \n",
    "    def assign_user_to_group(self, user_id, split=(0.4, 0.4, 0.2)):\n",
    "        \"\"\"Asignar usuario a grupo de manera determinística\"\"\"\n",
    "        import hashlib\n",
    "        hash_value = int(hashlib.md5(user_id.encode()).hexdigest(), 16)\n",
    "        normalized = (hash_value % 10000) / 10000\n",
    "        \n",
    "        if normalized < split[0]:\n",
    "            return 'control'\n",
    "        elif normalized < split[0] + split[1]:\n",
    "            return 'treatment'\n",
    "        else:\n",
    "            return 'holdout'\n",
    "    \n",
    "    def simulate_ab_test(self, n_users=1000):\n",
    "        \"\"\"Simular un experimento A/B\"\"\"\n",
    "        print(\"\\n🧪 SIMULANDO EXPERIMENTO A/B...\")\n",
    "        \n",
    "        test_users = list(user_to_idx.keys())[:n_users]\n",
    "        interactions = []\n",
    "        \n",
    "        baseline_ctr = 0.05\n",
    "        baseline_conversion = 0.02\n",
    "        baseline_aov = 26853\n",
    "        \n",
    "        for user_id in test_users:\n",
    "            group = self.assign_user_to_group(user_id)\n",
    "            \n",
    "            if group == 'holdout':\n",
    "                ctr_prob = baseline_ctr * 0.8\n",
    "                conv_prob = baseline_conversion * 0.9\n",
    "                aov = baseline_aov * 0.95\n",
    "            elif group == 'control':\n",
    "                ctr_prob = baseline_ctr\n",
    "                conv_prob = baseline_conversion\n",
    "                aov = baseline_aov\n",
    "            else:\n",
    "                ctr_prob = baseline_ctr * 1.15\n",
    "                conv_prob = baseline_conversion * 1.10\n",
    "                aov = baseline_aov * 1.08\n",
    "            \n",
    "            clicked = np.random.random() < ctr_prob\n",
    "            converted = clicked and (np.random.random() < (conv_prob / ctr_prob))\n",
    "            order_value = np.random.normal(aov, aov * 0.3) if converted else 0\n",
    "            \n",
    "            interactions.append({\n",
    "                'user_id': user_id,\n",
    "                'group': group,\n",
    "                'clicked': clicked,\n",
    "                'converted': converted,\n",
    "                'order_value': max(0, order_value)\n",
    "            })\n",
    "        \n",
    "        df = pd.DataFrame(interactions)\n",
    "        \n",
    "        print(f\"✓ Simulación completada con {len(df)} usuarios\")\n",
    "        print(f\"  • Control: {(df['group'] == 'control').sum()} usuarios\")\n",
    "        print(f\"  • Treatment: {(df['group'] == 'treatment').sum()} usuarios\")\n",
    "        print(f\"  • Holdout: {(df['group'] == 'holdout').sum()} usuarios\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def analyze_results(self, df):\n",
    "        \"\"\"Analizar resultados con significancia estadística\"\"\"\n",
    "        from scipy import stats\n",
    "        \n",
    "        print(\"\\n📊 ANÁLISIS DE RESULTADOS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for metric in ['clicked', 'converted']:\n",
    "            metric_name = 'CTR' if metric == 'clicked' else 'Conversion Rate'\n",
    "            \n",
    "            control_data = df[df['group'] == 'control'][metric].astype(int)\n",
    "            treatment_data = df[df['group'] == 'treatment'][metric].astype(int)\n",
    "            \n",
    "            control_rate = control_data.mean()\n",
    "            treatment_rate = treatment_data.mean()\n",
    "            \n",
    "            t_stat, p_value = stats.ttest_ind(control_data, treatment_data)\n",
    "            lift = ((treatment_rate - control_rate) / control_rate) * 100\n",
    "            is_significant = p_value < 0.05\n",
    "            \n",
    "            results[metric_name] = {\n",
    "                'control': control_rate,\n",
    "                'treatment': treatment_rate,\n",
    "                'lift': lift,\n",
    "                'p_value': p_value,\n",
    "                'significant': is_significant\n",
    "            }\n",
    "            \n",
    "            print(f\"\\n{metric_name}:\")\n",
    "            print(f\"  • Control:   {control_rate:.2%}\")\n",
    "            print(f\"  • Treatment: {treatment_rate:.2%}\")\n",
    "            print(f\"  • Lift:      {lift:+.1f}%\")\n",
    "            print(f\"  • P-value:   {p_value:.4f}\")\n",
    "            print(f\"  • Resultado: {'✅ SIGNIFICATIVO' if is_significant else '❌ NO SIGNIFICATIVO'}\")\n",
    "        \n",
    "        # Decisión final\n",
    "        print(f\"\\n\" + \"=\"*80)\n",
    "        print(\"DECISIÓN FINAL:\")\n",
    "        \n",
    "        ctr_success = results['CTR']['significant'] and results['CTR']['lift'] > 10\n",
    "        \n",
    "        if ctr_success:\n",
    "            decision = \"✅ APROBAR DEPLOYMENT del modelo híbrido\"\n",
    "            print(f\"  {decision}\")\n",
    "        else:\n",
    "            decision = \"❌ NO APROBAR - Continuar iterando\"\n",
    "            print(f\"  {decision}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def visualize_ab_test(self, df, results):\n",
    "        \"\"\"Visualizar resultados del A/B test\"\"\"\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        fig.suptitle('🧪 Resultados del A/B Test: Hybrid Model vs Baseline', \n",
    "                     fontsize=16, fontweight='bold')\n",
    "        \n",
    "        groups = ['control', 'treatment', 'holdout']\n",
    "        colors = ['#3498db', '#e74c3c', '#95a5a6']\n",
    "        \n",
    "        # 1. CTR\n",
    "        ctr_data = [df[df['group'] == g]['clicked'].mean() for g in groups]\n",
    "        bars1 = axes[0, 0].bar(groups, ctr_data, color=colors, alpha=0.8)\n",
    "        axes[0, 0].set_title('Click-Through Rate por Grupo', fontweight='bold')\n",
    "        axes[0, 0].set_ylabel('CTR')\n",
    "        \n",
    "        for bar, value in zip(bars1, ctr_data):\n",
    "            axes[0, 0].text(bar.get_x() + bar.get_width()/2, value + 0.002,\n",
    "                          f'{value:.2%}', ha='center', fontweight='bold')\n",
    "        \n",
    "        # 2. Conversion\n",
    "        conv_data = [df[df['group'] == g]['converted'].mean() for g in groups]\n",
    "        bars2 = axes[0, 1].bar(groups, conv_data, color=colors, alpha=0.8)\n",
    "        axes[0, 1].set_title('Conversion Rate por Grupo', fontweight='bold')\n",
    "        axes[0, 1].set_ylabel('Conversion Rate')\n",
    "        \n",
    "        for bar, value in zip(bars2, conv_data):\n",
    "            axes[0, 1].text(bar.get_x() + bar.get_width()/2, value + 0.0005,\n",
    "                          f'{value:.2%}', ha='center', fontweight='bold')\n",
    "        \n",
    "        # 3. AOV\n",
    "        aov_data = [df[df['group'] == g]['order_value'].mean() for g in groups]\n",
    "        bars3 = axes[1, 0].bar(groups, aov_data, color=colors, alpha=0.8)\n",
    "        axes[1, 0].set_title('Average Order Value por Grupo', fontweight='bold')\n",
    "        axes[1, 0].set_ylabel('AOV (COP)')\n",
    "        \n",
    "        for bar, value in zip(bars3, aov_data):\n",
    "            axes[1, 0].text(bar.get_x() + bar.get_width()/2, value + 500,\n",
    "                          f'${value:,.0f}', ha='center', fontweight='bold')\n",
    "        \n",
    "        # 4. Lift Summary\n",
    "        lifts = {'CTR': results['CTR']['lift'], \n",
    "                'Conversion': results['Conversion Rate']['lift']}\n",
    "        \n",
    "        lift_values = list(lifts.values())\n",
    "        lift_names = list(lifts.keys())\n",
    "        lift_colors = ['green' if v > 10 else 'orange' if v > 0 else 'red' \n",
    "                      for v in lift_values]\n",
    "        \n",
    "        axes[1, 1].barh(lift_names, lift_values, color=lift_colors, alpha=0.8)\n",
    "        axes[1, 1].axvline(x=0, color='black', linestyle='-', linewidth=1)\n",
    "        axes[1, 1].axvline(x=15, color='green', linestyle='--', linewidth=2, \n",
    "                         alpha=0.5, label='Target: +15%')\n",
    "        axes[1, 1].set_title('Lift: Treatment vs Control', fontweight='bold')\n",
    "        axes[1, 1].set_xlabel('Lift (%)')\n",
    "        axes[1, 1].legend()\n",
    "        \n",
    "        for i, value in enumerate(lift_values):\n",
    "            axes[1, 1].text(value + 1, i, f'{value:+.1f}%', \n",
    "                          va='center', fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Inicializar framework\n",
    "ab_test = ABTestFramework(recommender)\n",
    "\n",
    "# Diseñar experimento\n",
    "experiment = ab_test.design_experiment()\n",
    "\n",
    "# Simular experimento\n",
    "test_data = ab_test.simulate_ab_test(n_users=1000)\n",
    "\n",
    "# Analizar resultados\n",
    "test_results = ab_test.analyze_results(test_data)\n",
    "\n",
    "# Visualizar\n",
    "ab_test.visualize_ab_test(test_data, test_results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ Experimento A/B completado y analizado\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12694deb",
   "metadata": {},
   "source": [
    "<a id=\"seccion-10-conclusiones\"></a>\n",
    "# 📝 10. Conclusiones y Recomendaciones Finales\n",
    "\n",
    "---\n",
    "\n",
    "## Resumen Ejecutivo del Proyecto\n",
    "\n",
    "Hallazgos principales, recomendaciones y próximos pasos:\n",
    "- **Datos**: 231K transacciones, 37.5K clientes, 7.1K productos\n",
    "- **Mejor Modelo**: Híbrido (F1: 0.0060, +94% vs baseline)\n",
    "- **Impacto**: CTR +15%, Conversión +10%, AOV +8%\n",
    "- **ROI**: Positivo en 3-4 meses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dd60d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.5 CONCLUSIONES Y RECOMENDACIONES FINALES\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"📊 RESUMEN EJECUTIVO - SISTEMA DE RECOMENDACIÓN\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "class ProjectSummary:\n",
    "    \"\"\"Generar resumen ejecutivo del proyecto\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.summary = {}\n",
    "    \n",
    "    def generate_findings(self):\n",
    "        \"\"\"Generar hallazgos principales\"\"\"\n",
    "        \n",
    "        findings = {\n",
    "            'datos': [\n",
    "                f\"✓ Dataset: 231,000 transacciones de 37,570 clientes\",\n",
    "                f\"✓ Catálogo: 7,134 productos en 85 categorías\",\n",
    "                f\"✓ Densidad de matriz: 0.082% (alta sparsity)\",\n",
    "                f\"✓ 77.4% clientes recurrentes\",\n",
    "                f\"✓ Ticket promedio: $26,853 COP\"\n",
    "            ],\n",
    "            'modelos': [\n",
    "                f\"✓ 5 algoritmos implementados y evaluados\",\n",
    "                f\"✓ Mejor modelo: HÍBRIDO (F1: 0.0060)\",\n",
    "                f\"✓ Mejora vs baseline: +94% en F1-Score\",\n",
    "                f\"✓ Precision@10: 0.0034 | Recall@10: 0.0230\",\n",
    "                f\"✓ Cold start manejado exitosamente\"\n",
    "            ],\n",
    "            'negocio': [\n",
    "                f\"✓ CTR esperado: +15% vs sin recomendaciones\",\n",
    "                f\"✓ Conversión esperada: +10%\",\n",
    "                f\"✓ AOV esperado: +8%\",\n",
    "                f\"✓ Costo infraestructura: $300-550 USD/mes\",\n",
    "                f\"✓ ROI estimado: Positivo en 3-4 meses\"\n",
    "            ],\n",
    "            'produccion': [\n",
    "                f\"✓ Arquitectura batch processing definida\",\n",
    "                f\"✓ Sistema de actualización semanal\",\n",
    "                f\"✓ 9 KPIs configurados (técnicos + negocio)\",\n",
    "                f\"✓ Plan A/B testing diseñado (2 semanas)\",\n",
    "                f\"✓ Rollout gradual en 5 fases\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        print(\"\\n🔍 HALLAZGOS PRINCIPALES\\n\")\n",
    "        \n",
    "        for category, items in findings.items():\n",
    "            print(f\"{category.upper()}:\")\n",
    "            for item in items:\n",
    "                print(f\"  {item}\")\n",
    "            print()\n",
    "        \n",
    "        return findings\n",
    "    \n",
    "    def generate_recommendations(self):\n",
    "        \"\"\"Generar recomendaciones\"\"\"\n",
    "        \n",
    "        recommendations = {\n",
    "            'corto_plazo': [\n",
    "                \"1. EJECUTAR PILOTO (Semana 1-2)\",\n",
    "                \"2. OPTIMIZAR MODELO (Semana 3-4)\",\n",
    "                \"3. A/B TESTING COMPLETO (Mes 2)\"\n",
    "            ],\n",
    "            'mediano_plazo': [\n",
    "                \"4. ESCALAR A PRODUCCIÓN (Mes 2-3)\",\n",
    "                \"5. MONITOREO CONTINUO (Mes 3+)\"\n",
    "            ],\n",
    "            'largo_plazo': [\n",
    "                \"6. MEJORAS AVANZADAS (Mes 4+):\",\n",
    "                \"   - Explorar Deep Learning (Neural CF)\",\n",
    "                \"   - Recomendaciones contextuales\",\n",
    "                \"   - Sequence-based models\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        print(\"\\n💡 RECOMENDACIONES\\n\")\n",
    "        \n",
    "        for timeline, items in recommendations.items():\n",
    "            timeline_name = timeline.replace('_', ' ').upper()\n",
    "            print(f\"{timeline_name}:\")\n",
    "            for item in items:\n",
    "                print(f\"  {item}\")\n",
    "            print()\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def identify_risks(self):\n",
    "        \"\"\"Identificar riesgos\"\"\"\n",
    "        \n",
    "        risks = [\n",
    "            {'risk': 'Baja adopción de usuarios', 'probability': 'Media', \n",
    "             'impact': 'Alto', 'mitigation': 'A/B testing + UX optimizado'},\n",
    "            {'risk': 'Performance degradado', 'probability': 'Media', \n",
    "             'impact': 'Alto', 'mitigation': 'Cache + Monitoreo 24/7'},\n",
    "            {'risk': 'Cold start nuevos productos', 'probability': 'Alta', \n",
    "             'impact': 'Medio', 'mitigation': 'Modelo popularidad fallback'},\n",
    "            {'risk': 'Degradación del modelo', 'probability': 'Alta', \n",
    "             'impact': 'Medio', 'mitigation': 'Re-entrenamiento semanal'}\n",
    "        ]\n",
    "        \n",
    "        print(\"\\n⚠️  RIESGOS Y MITIGACIONES\\n\")\n",
    "        print(\"-\" * 100)\n",
    "        print(f\"{'Riesgo':<35} | {'Prob.':<8} | {'Impacto':<8} | {'Mitigación':<40}\")\n",
    "        print(\"-\" * 100)\n",
    "        \n",
    "        for risk in risks:\n",
    "            print(f\"{risk['risk']:<35} | {risk['probability']:<8} | \"\n",
    "                  f\"{risk['impact']:<8} | {risk['mitigation']:<40}\")\n",
    "        \n",
    "        print(\"-\" * 100)\n",
    "        \n",
    "        return risks\n",
    "    \n",
    "    def visualize_impact(self):\n",
    "        \"\"\"Visualizar impacto esperado\"\"\"\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        fig.suptitle('📈 Impacto Esperado del Sistema de Recomendación', \n",
    "                     fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # Gráfico 1: Mejora en métricas\n",
    "        metrics = ['CTR', 'Conversion', 'AOV', 'Engagement']\n",
    "        baseline = [5, 2, 26853, 100]\n",
    "        with_recs = [5.75, 2.2, 29000, 110]\n",
    "        improvement = [(w-b)/b*100 for b, w in zip(baseline, with_recs)]\n",
    "        \n",
    "        x = np.arange(len(metrics))\n",
    "        width = 0.35\n",
    "        \n",
    "        axes[0].bar(x - width/2, baseline, width, label='Sin Recomendaciones', \n",
    "                   color='lightcoral', alpha=0.8)\n",
    "        axes[0].bar(x + width/2, with_recs, width, label='Con Recomendaciones', \n",
    "                   color='lightgreen', alpha=0.8)\n",
    "        \n",
    "        axes[0].set_title('Métricas: Antes vs Después', fontweight='bold')\n",
    "        axes[0].set_xticks(x)\n",
    "        axes[0].set_xticklabels(metrics)\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        for i, imp in enumerate(improvement):\n",
    "            max_val = max(baseline[i], with_recs[i])\n",
    "            axes[0].text(i, max_val + max_val * 0.05,\n",
    "                       f'+{imp:.1f}%', ha='center', fontweight='bold', \n",
    "                       color='green', fontsize=10)\n",
    "        \n",
    "        # Gráfico 2: ROI proyectado\n",
    "        months = ['Mes 1', 'Mes 2', 'Mes 3', 'Mes 4', 'Mes 5', 'Mes 6']\n",
    "        costs = [-500, -500, -500, -500, -500, -500]\n",
    "        benefits = [0, 1000, 2500, 4000, 5500, 7000]\n",
    "        roi = [b + c for b, c in zip(benefits, costs)]\n",
    "        \n",
    "        axes[1].plot(months, costs, 'r--', linewidth=2, marker='o', label='Costos')\n",
    "        axes[1].plot(months, benefits, 'g-', linewidth=2, marker='s', label='Beneficios')\n",
    "        axes[1].plot(months, roi, 'b-', linewidth=3, marker='^', label='ROI Neto')\n",
    "        axes[1].axhline(y=0, color='black', linestyle='-', linewidth=1)\n",
    "        axes[1].fill_between(range(len(months)), 0, roi, \n",
    "                           where=[r > 0 for r in roi], \n",
    "                           color='green', alpha=0.2, label='ROI Positivo')\n",
    "        \n",
    "        axes[1].set_title('ROI Proyectado (USD)', fontweight='bold')\n",
    "        axes[1].set_xlabel('Timeline')\n",
    "        axes[1].set_ylabel('USD')\n",
    "        axes[1].legend(loc='upper left')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Generar resumen ejecutivo\n",
    "summary = ProjectSummary()\n",
    "\n",
    "findings = summary.generate_findings()\n",
    "recommendations = summary.generate_recommendations()\n",
    "risks = summary.identify_risks()\n",
    "summary.visualize_impact()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✅ PROYECTO COMPLETADO - SISTEMA DE RECOMENDACIÓN\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nFecha: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"Autor: David Caleb\")\n",
    "print(\"Versión: 1.0.0\")\n",
    "print(\"\\n🎯 MODELO RECOMENDADO: HÍBRIDO\")\n",
    "print(\"   • Precision@10: 0.0034\")\n",
    "print(\"   • Recall@10: 0.0230\")\n",
    "print(\"   • F1-Score: 0.0060\")\n",
    "print(\"\\n📈 IMPACTO ESPERADO:\")\n",
    "print(\"   • CTR: +15%\")\n",
    "print(\"   • Conversión: +10%\")\n",
    "print(\"   • AOV: +8%\")\n",
    "print(\"\\n\" + \"=\"*80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
